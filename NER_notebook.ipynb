{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff0c347-ee0f-4d11-9720-c3936ccc5bce",
   "metadata": {},
   "source": [
    "# Ноутбук для третьего задания по курсу \"Обработка текстов\" 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe110105-e44c-4d37-86db-42719d14e539",
   "metadata": {},
   "source": [
    "## Работа с датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de209159-5e0e-4e2d-8142-59c283dd9c4e",
   "metadata": {},
   "source": [
    "### Работа с файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5e74f4-3461-472c-b476-723775f208b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка для отрезков [start, end] (глупая за O(n^2))\n",
    "def bubble_sort(arr):\n",
    "    for i in range(len(arr) - 1):\n",
    "        for j in range(len(arr) - i - 1):\n",
    "            if arr[j][0] > arr[j + 1][0] or arr[j][0] == arr[j + 1][0] and arr[j][1] < arr[j + 1][1]:\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "    return arr\n",
    "\n",
    "\n",
    "# Проверка на пересечение двух отсортированных отрезков\n",
    "def check_intersection(l1, r1, l2, r2):\n",
    "    return l2 <= r1\n",
    "                \n",
    "\n",
    "# Работа с файлами .ann формата\n",
    "def read_ann_file(path):\n",
    "    spans = []\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    for line in file:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if line[0] != 'T':\n",
    "            continue\n",
    "        \n",
    "        columns = line.split('\\t')\n",
    "        if ';' not in columns[1]:\n",
    "            category, start, end = columns[1].split(' ')\n",
    "            spans.append([int(start), int(end), category])\n",
    "    file.close()\n",
    "    \n",
    "    spans = bubble_sort(spans)\n",
    "    last_not_intersect = [-1, -1]\n",
    "    filtered_spans = []\n",
    "    for elem in spans:\n",
    "        if not check_intersection(last_not_intersect[0], last_not_intersect[1], elem[0], elem[1]):\n",
    "            last_not_intersect = [elem[0], elem[1]]\n",
    "            filtered_spans.append(elem)\n",
    "    \n",
    "    return filtered_spans\n",
    "    \n",
    "\n",
    "# Работа с файлами .txt формата\n",
    "def read_text_file(path):\n",
    "    with open(path) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1ab21-c7bb-4d40-a045-23b5bedf1a58",
   "metadata": {},
   "source": [
    "### BIO разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917b3921-e257-4cc5-8ad4-6624f2e8c69c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xellar/Projects/NERtask/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification, BertForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "def bio(path):\n",
    "    ann_file = read_ann_file(f'{path}.ann')\n",
    "    txt_file = read_text_file(f'{path}.txt')\n",
    "    spans = tokenizer(txt_file, return_offsets_mapping=True, add_special_tokens=False, truncation=True).offset_mapping\n",
    "    tokens = []\n",
    "    \n",
    "    for span in spans:\n",
    "        tokens.append(txt_file[span[0]:span[1]])\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    idx = 0\n",
    "    cnt_spans = len(spans)\n",
    "    for ann in ann_file:\n",
    "        while idx < cnt_spans and ann[0] > spans[idx][0]:\n",
    "            idx += 1\n",
    "        if idx < cnt_spans:\n",
    "            labels[idx] = f'B-{ann[2]}'\n",
    "        idx += 1\n",
    "        while idx < cnt_spans and spans[idx][1] <= ann[1]:\n",
    "            labels[idx] = f\"I-{ann[2]}\"\n",
    "            idx += 1\n",
    "    \n",
    "    return tokens, labels, spans\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4957c1-ac13-409e-a6ca-a12c8e962767",
   "metadata": {},
   "source": [
    "### Функция для чтения датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac650abf-a76c-42d5-b6b1-e87000a4a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    token_seq, label_seq, spans_seq = [], [], []\n",
    "    files = set()\n",
    "    for file in os.listdir(path):\n",
    "        if file[0] != '.':\n",
    "            files.add(file[:-4])\n",
    "    \n",
    "    for file in files:\n",
    "        tokens, labels, spans = bio(f'{path}/{file}')\n",
    "        token_seq.append(tokens)\n",
    "        label_seq.append(labels)\n",
    "        spans_seq.append(spans)\n",
    "    \n",
    "    return token_seq, label_seq, spans_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5cdde5-c271-4bf2-80c6-964cc29198dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Египет', 'ского', 'студента', 'могут', 'выслать', 'из', 'страны', 'за', 'высказывания', 'о', 'Трамп', 'е', 'Дональд', 'Трамп', 'Египет', 'ский', 'студент', ',', 'обуч', 'ающийся', 'в', 'американской', 'лет', 'ной', 'школе', ',', 'в', 'пятницу', ',', '4', 'марта', '2016', 'года', ',', 'вынужден', 'был', 'явиться', 'в', 'иммигра', 'ционный', 'суд', ',', 'где', 'будет', 'приниматься', 'решение', 'о', 'его', 'депортации', 'в', 'связи', 'с', 'опубликованным', 'и', 'им', 'в', 'социальных', 'сетях', 'комментариями', ',', 'содержа', 'щими', 'угрозы', 'в', 'адрес', 'потенциального', 'кандидата', 'в', 'президенты', 'от', 'Республиканской', 'партии', 'Дональда', 'Трампа', '.', 'Эм', 'аде', 'льд', 'ин', 'Эль', 'са', 'ед', ',', '23', '-', 'летний', 'студент', 'из', 'Каи', 'ра', ',', 'предстал', 'перед', 'иммигра', 'ционным', 'судом', 'в', 'Лос', '-', 'Анджелесе', ',', 'после', 'того', 'как', 'он', 'написал', 'на', 'своей', 'странице', 'в', 'Facebook', ',', 'что', 'готов', 'отсид', 'еть', 'пожиз', 'ненный', 'срок', 'за', 'убийство', 'Трампа', 'и', 'уверен', ',', 'что', 'мир', 'будет', 'благодарен', 'ему', 'за', 'это', '.', 'Представители', 'иммигра', 'ционных', 'властей', 'поместили', 'Эль', 'са', 'еда', 'в', 'след', 'ственный', 'изолятор', 'округа', 'Ори', 'нд', 'ж', ',', 'где', 'он', 'содержится', 'с', '12', 'февраля', '.', 'Как', 'сообщил', 'репорт', 'ерам', 'адвокат', 'Эль', 'са', 'еда', ',', 'совершенно', 'очевидно', ',', 'что', 'его', 'подзащи', 'тный', 'не', 'угрожал', 'никому', 'всерьез', '.', 'После', 'того', 'как', 'в', 'начале', 'февраля', 'Эль', 'са', 'ед', 'опубликовал', 'свои', 'комментарии', ',', 'его', 'допрос', 'или', 'агенты', 'Секрет', 'ной', 'службы', '.', 'Ему', 'сообщили', ',', 'что', 'обвинение', 'в', 'совершении', 'преступления', ',', 'предусмотренного', 'федеральным', 'законом', ',', 'ему', 'предъявлено', 'не', 'будет', ',', 'но', 'его', 'студенче', 'ская', 'виза', 'будет', 'аннул', 'ирована', '.', 'Владелец', 'лет', 'ной', 'школы', 'сообщил', 'репорт', 'ерам', ',', 'что', 'по', 'требованию', 'федеральных', 'властей', 'он', 'лишил', 'Эль', 'са', 'еда', 'права', 'посещать', 'его', 'школу', '.', 'Он', 'также', 'сказал', ',', 'что', 'готов', 'в', 'любой', 'момент', 'восстановить', 'Эль', 'са', 'еда', ',', 'если', 'будет', 'получено', 'соответствующее', 'разрешение', 'от', 'властей', '.', 'Эль', 'са', 'ед', 'сказал', ',', 'что', 'опубликовал', 'эти', 'комментарии', ',', 'после', 'того', 'как', 'скандал', 'ьный', 'кандидат', 'призвал', 'запретить', 'въезд', 'мусульман', 'в', 'Соединенные', 'Штаты', '.', 'Эль', 'са', 'ед', 'сказал', ',', 'что', 'был', 'возму', 'щен', 'этими', 'высказывания', 'ми', 'Трампа', ',', 'но', 'не', 'собирался', 'никому', 'причин', 'ять', 'никакого', 'вреда', '.'], ['Премьер', 'Палестины', 'Хам', 'дал', 'ла', 'подал', 'в', 'отставку', 'Источник', 'в', 'правительстве', 'сообщил', ',', 'что', 'Хам', 'дал', 'ла', 'не', 'пола', 'дил', 'с', 'двумя', 'вице', '-', 'премьера', 'ми', '.', 'Рами', 'Хам', 'дал', 'ла', 'Новый', 'премьер', '-', 'министр', 'Палестины', 'Рами', 'Хам', 'дал', 'ла', ',', 'назначенный', '2', 'июня', ',', 'подал', 'в', 'отставку', ',', 'сообщает', 'агентство', 'Рейт', 'ер', 'со', 'ссылкой', 'на', 'источник', 'в', 'палестин', 'ском', 'правительстве', '.', 'Пока', 'неизвестно', ',', 'принял', 'ли', 'отставку', 'глава', 'Палес', 'тинской', 'национальной', 'администрации', '(', 'П', 'НА', ')', 'Махмуд', 'Аббас', '.', '\"', 'Хам', 'дал', 'ла', 'подал', 'прошение', 'об', 'отставке', 'из', '-', 'за', 'расхо', 'ждений', 'с', 'двумя', 'вице', '-', 'премьера', 'ми', '\"', ',', '—', 'сообщил', 'источник', 'агентству', 'Франс', 'Пресс', '.', 'Бывший', 'университет', 'ский', 'ректор', 'Рами', 'Хам', 'дал', 'ла', '6', 'июня', 'сформировал', 'новое', 'палестин', 'ское', 'правительство', 'и', 'принял', 'присягу', '.', 'Он', 'неоднократно', 'выражал', 'надежду', 'на', 'то', ',', 'что', 'его', 'кабинет', 'просу', 'ществу', 'ет', 'до', 'середины', 'августа', 'и', 'уступ', 'ит', 'место', 'правительству', 'национального', 'единства', ',', 'над', 'созданием', 'которого', 'работают', 'конкури', 'рующие', 'движения', 'Ф', 'АТ', 'Х', 'и', 'Х', 'АМ', 'АС', '.', 'Хам', 'дал', 'ла', ',', 'которому', 'в', 'августе', 'исполнится', '55', 'лет', ',', 'стал', 'премьером', 'после', 'того', ',', 'как', 'в', 'середине', 'апреля', 'в', 'отставку', 'подал', 'популярный', 'на', 'Западе', ',', 'но', 'критику', 'емый', 'на', 'родине', 'премьер', '-', 'министр', 'Сал', 'ам', 'Ф', 'ая', 'д', '.', 'Ф', 'ая', 'да', 'в', 'Палес', 'тине', 'критиковали', 'за', 'падение', 'темпов', 'экономического', 'роста', ',', 'всплеск', 'безработицы', ',', 'нехватку', 'денег', 'в', 'каз', 'не', 'и', 'попытки', 'компенсировать', 'бюджетный', 'дефицит', 'непопуля', 'рными', 'мерами', 'экономии', ',', 'которые', 'спровоцировал', 'и', 'антип', 'равитель', 'ственные', 'выступления', '.', 'Палес', 'тинский', 'кабинет', 'пользуется', 'поддержкой', 'мирового', 'сообщества', 'и', 'фактически', 'функционирует', 'только', 'в', 'пределах', 'Западного', 'берега', 'реки', 'Иордан', '.'], ['Харьков', 'чан', 'пока', 'тали', 'на', 'воздушном', 'шар', 'е', 'мини', '|', 'слева', '19', 'октября', '2012', 'года', 'в', 'Харькове', 'отмечали', 'День', 'Московского', 'района', ',', 'которому', 'исполнилось', '75', 'лет', '.', 'Праздник', 'для', 'жителей', 'Салт', 'овки', 'организовали', 'фонд', '«', 'Перес', 'вет', '»', 'при', 'поддержке', 'администрации', 'Московского', 'района', '.', 'Торж', 'ества', 'начались', 'с', 'финала', 'конкурса', '«', 'ДС', '-', 'Фактор', '»', ',', 'затем', 'вы', 'лились', 'на', 'улицу', ',', 'где', 'са', 'лт', 'ов', 'чан', 'порад', 'овали', 'возможностью', 'покататься', 'на', 'воздушном', 'шар', 'е', '.', 'Желаю', 'щие', 'могли', 'подняться', 'на', 'высоту', 'птичьего', 'полета', 'и', 'полюбоваться', 'пейзажа', 'ми', 'родного', 'города', '.', 'Для', 'детей', 'был', 'построен', 'целый', 'детский', 'городок', 'со', 'множеством', 'карус', 'елей', 'и', 'захват', 'ывающих', 'аттракционов', ',', 'для', 'взрослых', 'было', 'организовано', 'чае', 'пити', 'е', 'и', 'танцы', 'под', 'духов', 'ой', 'оркестр', '.', 'Торжественное', 'мероприятие', 'окончил', 'ось', 'концертом', 'в', 'котором', 'приняли', 'участие', 'финал', 'исты', 'конкурса', 'талантов', '«', 'ДС', '-', 'Фактор', '»', '.', 'Иници', 'ированный', 'Дмитрием', 'Свят', 'аш', 'ем', 'конкурс', 'длился', 'два', 'года', ':', 'первый', 'отбор', 'очный', 'этап', 'проходил', 'с', 'сентября', '2011', 'по', 'апрель', '2012', ',', 'полуфинал', 'и', 'финал', '—', 'в', 'сентябре', '—', 'октябре', '2012', '.', 'В', 'нём', 'принимали', 'участие', 'все', 'желающие', 'школьники', '.', 'Конкурса', 'нты', 'представляли', 'на', 'суд', 'жюри', 'и', 'публики', 'по', 'одному', 'эстра', 'дному', 'номеру', 'в', 'любом', 'жанре', '.', 'Отбор', 'очные', 'этапы', 'проходили', 'в', 'школах', 'и', 'заканчивались', 'определением', 'трёх', 'победителей', 'и', 'обладателя', 'приза', 'зрительских', 'симпатий', 'в', 'каждой', 'школе', '.', 'Детей', ',', 'заняв', 'ших', 'призовые', 'места', ',', 'наградили', 'ценными', 'приза', 'ми', ':', 'планшет', 'ом', 'за', 'гран', '-', 'при', ',', 'ноутбук', 'ом', 'за', 'первое', 'место', ',', 'нет', 'бук', 'ом', ',', 'электронной', 'кни', 'жкой', 'и', 'фотоаппарат', 'ом', 'за', 'последующие', 'призовые', 'места', '.', 'Гран', '-', 'при', 'получила', 'вокалист', 'ка', 'Вероника', 'Карна', 'ух', 'ова', ',', 'а', 'школа', 'где', 'она', 'учится', ',', '—', '11', 'ноутбуков', 'для', 'компьютерного', 'класса', '.', '«', 'Этот', 'конкурс', 'очень', 'важен', 'для', 'детей', 'Салт', 'овки', ',', '—', 'комментирует', 'Дмитрий', 'Свят', 'аш', '.', '—', 'В', 'местной', 'администрации', 'и', 'в', 'фонде', '„', 'Перес', 'вет', '“', 'очень', 'хорошо', 'понимают', ',', 'что', 'в', 'школьном', 'возрасте', 'необходимо', 'начинать', 'достигать', 'определенных', 'успехов', 'для', 'того', ',', 'чтоб', 'быть', 'более', 'уверенным', 'в', 'себе', 'в', 'более', 'взрослом', 'возрасте', '.', 'Поэтому', 'мы', 'обязаны', 'обеспечить', 'детям', 'различные', 'платформы', 'для', 'саморе', 'ализации', '»', ',', '—', 'сказал', 'депутат', '.', 'Конкурс', 'был', 'посвящён', 'любимому', 'району', 'и', 'учитывая', 'успех', 'начинания', ',', 'решено', 'проводить', 'его', 'на', 'постоянной', 'основе', 'раз', 'в', '2', 'года', '.', 'Закончил', 'ся', 'праздник', 'великолеп', 'ным', 'фейервер', 'ком', '.'], ['Обама', 'может', 'посетить', 'тюрьму', ',', 'в', 'которой', 'отбывал', 'срок', 'Манд', 'ела', 'Обама', 'находится', 'с', 'визитом', 'в', 'ЮАР', '.', 'Ожидается', ',', 'что', 'президент', 'США', 'посетит', 'на', 'острове', 'Роб', 'бен', 'эй', 'ланд', 'тюрьму', ',', 'в', 'которой', 'находился', 'в', 'заключении', 'Манд', 'ела', ',', 'в', 'знак', 'уважения', 'к', 'тяжело', 'боль', 'ному', 'политику', '.', 'Президент', 'США', 'Барак', 'Обама', 'в', 'рамках', 'визита', 'в', 'ЮАР', 'в', 'воскресенье', 'посетил', 'остров', 'Роб', 'бен', 'эй', 'ланд', ',', 'где', 'расположена', 'тюрьма', ',', 'в', 'которой', 'долгие', 'годы', 'находился', 'в', 'заключении', 'самый', 'известный', 'в', 'мире', 'борец', 'против', 'ап', 'арт', 'еи', 'да', 'Нельсон', 'Манд', 'ела', ',', 'сообщает', 'агентство', 'Франс', 'Пресс', '.', 'Как', 'ожидается', ',', 'Обама', 'посетит', 'на', 'острове', 'тюрьму', 'и', 'бывшую', 'камеру', 'Манд', 'елы', 'в', 'знак', 'уважения', 'к', 'тяжело', 'боль', 'ному', '94', '-', 'летнему', 'политику', '.', 'Кам', 'енная', 'тюрьма', 'на', 'Роб', 'бен', 'эй', 'ланд', ',', 'где', 'с', '1962', 'по', '1990', 'год', 'содержал', 'ся', 'впоследствии', 'первый', 'черноко', 'жий', 'президент', 'ЮАР', ',', 'находится', 'в', '12', 'километрах', 'от', 'Кей', 'пта', 'уна', '.', 'Накануне', 'Обама', 'пообщался', 'с', 'семьей', 'Манд', 'елы', '.', 'Сам', 'Манд', 'ела', 'находится', 'в', 'критическом', 'состоянии', ',', 'и', 'Обама', 'заявлял', 'ранее', ',', 'что', 'не', 'уверен', 'в', 'возможности', 'личной', 'встречи', 'с', 'ним', '.', 'В', 'начале', 'июня', 'Манд', 'ела', 'попал', 'в', 'госпиталь', 'в', 'Прет', 'ории', 'с', 'рециди', 'вом', 'лего', 'чной', 'инфекции', '.', 'Несколько', 'дней', 'назад', 'его', 'состояние', 'стало', 'критическим', ',', 'хотя', 'затем', 'вновь', 'улучшилось', '.', 'В', 'субботу', 'президент', 'ЮАР', 'Джейкоб', 'Зу', 'ма', 'сообщил', ',', 'что', 'Манд', 'ела', 'по', '-', 'прежнему', 'находится', 'в', 'критическом', 'состоянии', ',', 'но', 'власти', 'надеются', ',', 'что', 'ему', 'станет', 'лучше', 'и', 'его', 'очень', 'скоро', 'вып', 'иш', 'ут', '.', 'Известный', 'борец', 'против', 'режима', 'ап', 'арт', 'еи', 'да', ',', 'Манд', 'ела', 'провел', 'в', 'тюрьме', '27', 'лет', '.', 'В', 'мае', '1994', 'года', 'он', 'стал', 'первым', 'черноко', 'жим', 'президентом', 'ЮАР', 'и', 'руководил', 'государством', 'до', 'июня', '1999', 'года', '.', 'В', '1993', 'году', 'был', 'награжден', 'Нобелевской', 'премией', 'мира', '.'], ['В', 'Роспотребнадзоре', 'хотят', 'штрафовать', 'россиян', 'за', 'хранение', 'и', 'перевозку', 'санк', 'ционных', 'продуктов', 'Пар', 'ме', 'зан', ',', 'созрев', 'ающий', 'на', 'сыров', 'ар', 'не', 'В', 'Роспотребнадзоре', 'предложили', 'штрафовать', 'россиян', 'за', 'хранение', 'и', 'перевозку', 'санк', 'ционных', 'продуктов', '.', 'Разработан', 'ные', 'ведомством', 'поправки', 'в', 'Кодекс', 'РФ', 'об', 'административных', 'правонарушениях', '(', 'в', 'статью', 'о', 'реализации', ',', 'хранении', 'и', 'транспорт', 'ировании', 'запрещён', 'ных', 'к', 'ввоз', 'у', 'в', 'РФ', 'товаров', ')', 'предполагают', 'максимальное', 'наказание', 'в', 'виде', 'штрафа', 'в', 'размере', '100', 'тысяч', 'рублей', '.', 'Соответствующий', 'проект', 'федерального', 'закона', 'опубликован', 'на', 'портале', 'проектов', 'нормативных', 'актов', '.', 'Гражда', 'н', 'предлагают', 'штрафовать', 'на', 'суммы', 'от', '3', 'до', '5', 'тыс', '.', 'руб', '.', ',', 'должностных', 'лиц', 'и', 'индивидуальных', 'предпринимателей', '—', 'от', '30', 'тыс', '.', 'до', '50', 'тыс', '.', 'руб', '.', ',', 'юридических', 'лиц', '—', 'от', '70', 'тыс', '.', 'до', '100', 'тыс', '.', 'руб', '.', 'Во', 'всех', 'случаях', 'предусматривается', 'также', 'возможность', 'конфиска', 'ции', 'санк', 'ционной', 'продукции', '.'], ['Тера', 'кт', 'в', 'Иерусалиме', ':', 'убиты', 'двое', 'полицейских', 'Погиб', 'шие', 'в', 'результате', 'теракта', 'полицейские', 'В', 'пятницу', ',', '14', 'июля', '2017', 'года', ',', 'в', 'Восточном', 'Иерусалиме', 'рядом', 'с', 'Храм', 'овой', 'горой', 'произошёл', 'теракт', '.', 'Утром', 'трое', 'неизвестных', 'неожиданно', 'начали', 'стрелять', 'возле', 'Ль', 'вины', 'х', 'ворот', 'Старого', 'города', ',', 'после', 'чего', 'попытались', 'убежать', 'и', 'скрыться', 'в', 'одной', 'из', 'меч', 'етей', 'на', 'Храм', 'овой', 'горе', '.', 'Ране', 'ных', 'полицейских', 'и', 'пограни', 'чника', 'госпитализировали', 'в', 'больницу', 'Ада', 'са', 'Хар', '-', 'ха', '-', 'Ц', 'офи', 'м', '(', 'בית', 'החולים', 'הדסה', 'הר', 'הצופים', ')', '.', 'Однако', 'двое', 'из', 'них', 'в', 'полдень', 'умерли', '.', 'После', 'нападения', 'вход', 'на', 'Храм', 'овую', 'гору', 'был', 'закрыт', ',', 'а', 'все', 'посетители', '—', 'эвакуированы', '.', 'После', '11', ':', '30', 'появились', 'сообщения', 'о', 'том', ',', 'что', 'воору', 'женное', 'нападение', 'на', 'Храм', 'овой', 'горе', 'совершили', 'трое', 'жителей', 'израильского', 'арабского', 'города', 'Ум', 'м', 'эль', '-', 'Фах', 'м', '.', 'Все', 'трое', ',', 'по', 'документам', ',', 'носили', 'одно', 'имя', '—', 'Мухаммад', 'Дж', 'аба', 'рин', '.', 'Им', 'было', '30', ',', '20', 'и', '19', 'лет', '.', 'Пол', 'ные', 'имена', 'террористов', ':', 'Мухаммад', 'Ахмад', 'Мухаммад', 'Дж', 'аба', 'рин', ',', 'Мухаммад', 'Хама', 'д', 'Аб', 'д', 'аль', '-', 'Ла', 'ти', 'ф', 'Дж', 'аба', 'рин', 'и', 'Мухаммад', 'Ахмад', 'Ма', 'фаль', 'Дж', 'аба', 'рин', '.', 'Двое', 'из', 'преступников', 'за', 'день', 'до', 'теракта', '13', 'июля', 'опубликовали', 'селфи', 'в', 'Facebook', 'с', 'подписью', ':', '«', 'Улы', 'бка', '.', 'Завтра', 'будет', 'лучше', '»', '.', 'Эта', 'фотография', 'также', 'опубликована', 'сайтом', '«', 'Палес', 'тинский', 'информационный', 'центр', '»', ',', 'принадлежа', 'щему', 'Х', 'АМ', 'АС', '.', 'Х', 'АМ', 'АС', 'не', 'берёт', 'на', 'себя', 'ответственность', 'за', 'этот', 'теракт', ',', 'утверждая', ',', 'что', 'трое', 'нападавших', 'были', 'активист', 'ами', 'северного', 'крыла', '«', 'Ислам', 'ского', 'движения', '»', 'и', 'м', 'стили', 'евреям', 'за', '«', 'осквер', 'нение', 'мечети', 'Аль', '-', 'Ак', 'са', '»', '.', 'В', 'связи', 'с', 'тем', ',', 'что', 'террориста', 'м', 'удалось', 'прон', 'ести', 'на', 'Храм', 'овую', 'гору', 'стрелков', 'ое', 'и', 'автоматическое', 'оружие', ',', 'начальник', 'полиции', 'Иерусалим', 'ского', 'округа', 'Йор', 'ам', 'Ха', '-', 'Леви', 'рекомендовал', 'правительству', 'установить', 'на', 'входа', 'х', 'на', 'Храм', 'овую', 'гору', 'рамки', 'металл', 'оиска', 'телей', 'В', 'пер', 'вые', 'за', '17', 'лет', 'у', 'входа', 'на', 'Храм', 'овую', 'гору', 'установлены', 'магнитные', 'ворота', '.', 'В', 'ночь', 'с', '15', 'на', '16', 'июля', 'в', 'столице', 'Иордан', 'ии', 'Ам', 'мане', 'прошли', 'многочисленные', 'демонстрации', 'с', 'требованиями', 'разорвать', 'мирный', 'договор', 'с', 'Израилем', 'из', '-', 'за', 'ситуации', 'на', 'Храм', 'овой', 'горе', '.', 'Иордан', 'цы', 'требуют', 'разорвать', 'мирное', 'соглашение', 'с', 'Израилем', '.'], ['Скончался', 'бывший', 'премьер', '-', 'министр', 'России', 'Виктор', 'Черном', 'ыр', 'дин', 'В', 'ночь', 'со', '2', '-', 'го', 'на', '3', '-', 'е', 'ноября', 'на', '73', '-', 'м', 'году', 'жизни', 'скончался', 'экс', '-', 'премьер', '-', 'министр', 'России', 'Виктор', 'Черном', 'ыр', 'дин', '.', 'Виктор', 'Черном', 'ыр', 'дин', ',', 'как', 'ожидается', ',', 'будет', 'похоронен', 'на', 'Новодевичьем', 'кладбище', '.', 'Виктор', 'Черном', 'ыр', 'дин', 'сделал', 'производственную', 'карьеру', 'в', 'газовой', 'промышленности', 'Советского', 'Союза', '.', 'В', 'конце', 'гор', 'ба', 'чев', 'ской', 'перестройки', '(', '1989', 'г', '.', ')', 'он', 'стал', 'председателем', 'правления', 'государственного', 'газового', 'концерна', '«', 'Газпром', '»', '.', 'В', 'начала', 'президентства', 'Бориса', 'Ельцина', '(', 'декабрь', '1992', 'г', '.', ')', 'Черном', 'ыр', 'дин', 'был', 'назначен', 'председателем', 'Совета', 'Министров', 'РФ', '.', 'Пост', 'российского', 'премьера', 'он', 'занимал', 'включительно', 'по', 'март', '1998', '-', 'го', 'года', '.', 'В', 'августе', '-', 'сентябре', '1998', 'г', '.', ',', 'во', 'время', 'правительственного', 'кризиса', ',', 'Черном', 'ыр', 'дин', 'вновь', 'исполнял', 'обязанности', 'председателя', 'правительства', 'РФ', '.', 'После', 'этого', 'кризиса', ',', 'уже', 'в', '1999', '-', 'м', ',', 'Черном', 'ыр', 'дин', 'вновь', 'возглавил', 'концерн', '«', 'Газпром', '»', '.', 'В', 'апреле', '1999', '-', 'го', ',', 'после', 'начала', 'военной', 'операции', 'НАТО', 'против', 'Югославии', ',', 'Виктор', 'Черном', 'ыр', 'дин', 'был', 'назначен', 'специальным', 'представителем', 'президента', 'РФ', 'по', 'урегулированию', 'конфликта', 'на', 'Балка', 'нах', '.', 'С', 'сентября', '2000', 'г', '.', 'экс', '-', 'премьер', 'стал', 'сопредседат', 'елем', 'попечитель', 'ского', 'совета', 'Россий', 'ско', '-', 'американского', 'совета', 'делового', 'сотрудничества', '.', 'В', '2001', '—', '2009', 'гг', '.', 'Виктор', 'Черном', 'ыр', 'дин', 'являлся', 'послом', 'РФ', 'в', 'Украине', ',', 'специальным', 'представителем', 'президента', 'РФ', 'по', 'торгово', '-', 'экономическим', 'связям', 'между', 'двумя', 'странами', 'Как', 'сообщает', 'сегодня', 'пресс', '-', 'служба', 'Кремля', ',', 'президент', 'России', 'Дмитрий', 'Медведев', 'выразил', 'глубокие', 'искренние', 'соболезнования', 'родным', 'и', 'близким', 'спец', 'представи', 'теля', 'главы', 'государства', 'по', 'вопросам', 'экономического', 'сотрудничества', 'с', 'государствами', '-', 'участниками', 'СНГ', 'Виктора', 'Черном', 'ыр', 'дина', 'в', 'связи', 'с', 'его', 'кон', 'чиной', '.', 'Свои', 'глубокие', 'соболезнования', 'в', 'связи', 'с', 'кон', 'чиной', 'Виктора', 'Черном', 'ыр', 'дина', 'также', 'выразил', 'сегодня', 'нынешний', 'глава', 'российского', 'правительства', 'Владимир', 'Путин', '.'], ['Путин', 'исключил', 'Гр', 'ыз', 'лова', 'из', 'Совета', 'Безопасности', 'Борис', 'Гр', 'ыз', 'лов', 'Президент', 'России', 'Владимир', 'Путин', 'исключил', 'Бориса', 'Гр', 'ыз', 'лова', 'из', 'состава', 'Совета', 'Безопасности', 'России', '.', 'Соответствующий', 'указ', 'опубликован', 'сегодня', 'на', 'официальном', 'интернет', '-', 'портале', 'правовой', 'информации', ':', 'Вне', 'сти', 'в', 'состав', 'Совета', 'Безопасности', 'Российской', 'Федерации', ',', 'утвержден', 'ный', 'указом', 'президента', 'Российской', 'Федерации', 'от', '25', 'мая', '2012', 'года', '…', 'изменение', ',', 'исключ', 'ив', 'из', 'него', 'Гр', 'ыз', 'лова', 'Бориса', 'Вячеслав', 'овича', 'О', 'мотива', 'х', 'и', 'подробно', 'стях', 'решения', 'не', 'сообщается', '.', '65', '-', 'летний', 'Борис', 'Гр', 'ыз', 'лов', '—', 'чиновник', 'из', '«', 'команды', '»', 'Путина', ',', 'занимавший', 'высшие', 'должности', 'в', 'России', '—', 'Министр', 'внутренних', 'дел', '(', '2001', '—', '2003', ')', ',', 'Председатель', 'Государственной', 'думы', 'четвёртого', 'и', 'пятого', 'созывов', '(', '2003', '—', '2011', ')', ',', 'Председатель', 'Высшего', 'совета', 'партии', '«', 'Единая', 'Россия', '»', '(', 'c', '2002', 'года', ')', ',', 'Полном', 'очный', 'представитель', 'России', 'в', 'Контак', 'тной', 'группе', 'по', 'урегулированию', 'ситуации', 'в', 'восточной', 'части', 'Украины', '.'], ['Правительство', 'Каталонии', 'отправлено', 'в', 'отставку', 'Карл', 'ес', 'Пуч', 'демон', 'провозгла', 'шает', 'курс', 'Каталонии', 'на', 'независимость', 'Председатель', 'кабинета', 'министров', 'Испании', 'Мари', 'ано', 'Рах', 'ой', 'отправил', 'Карл', 'еса', 'Пуч', 'демон', 'а', 'и', 'его', 'правительство', 'в', 'отставку', '.', 'До', 'сро', 'чные', 'выборы', 'нового', 'главы', 'региона', 'должны', 'состояться', 'в', 'течение', 'полугода', '.', 'Таким', 'образом', 'автономный', 'статус', 'Каталонии', 'был', 'приостановлен', 'и', ',', 'согласно', '155', 'статье', 'конституции', 'Испании', ',', 'над', 'регионом', 'было', 'введено', 'прямое', 'управление', '.', 'Это', 'решение', 'было', 'принято', 'на', 'заседании', 'кабинета', 'министров', 'в', 'субботу', ',', '21', 'октября', '.', 'Таким', 'стал', 'ответ', 'официального', 'Мадрид', 'а', 'на', 'Каталон', 'ский', 'референдум', ',', 'на', 'котором', 'более', '90', '%', 'проголосова', 'вших', 'высказались', 'за', 'независимость', 'региона', '.', 'Испан', 'ские', 'власти', 'неоднократно', 'заявляли', 'о', 'планах', 'решить', '«', 'каталон', 'ский', 'вопрос', '»', ',', 'не', 'выходя', 'за', 'рамки', 'законов', 'страны', '.', 'При', 'этом', 'Рах', 'ой', 'уточнил', ',', 'что', 'это', 'не', 'является', 'покушение', 'м', 'на', 'автоном', 'ию', 'Каталонии', ',', 'было', 'лишь', 'низ', 'ложен', 'о', 'правительство', ',', 'наруш', 'ив', 'шее', 'законы', 'страны', '.'], ['Ричард', 'Бёр', 'тон', 'получил', 'свою', 'звезду', 'на', 'Ал', 'лее', 'славы', 'в', 'Голливуде', 'Ричард', 'Бёр', 'тон', 'Всемир', 'но', 'известный', 'актер', 'Ричард', 'Бёр', 'тон', 'получил', 'свою', 'звезду', 'на', 'Ал', 'лее', 'славы', 'в', 'Голливуде', ',', 'рядом', 'с', 'Элизабет', 'Тейлор', ',', 'на', 'которой', 'был', 'женат', 'дважды', '.', 'На', 'церемонии', 'присутствовала', 'его', 'дочь', 'Мария', 'и', 'друг', 'Барто', 'на', 'актер', 'Майкл', 'Шин', '.', 'Ричард', 'Бёр', 'тон', '(', 'Richard', 'Bu', 'rton', ')', '—', 'британский', 'актёр', ',', 'семи', 'кратный', 'номинан', 'т', 'на', 'премию', '«', 'Оскар', '»', ';', 'обладатель', '«', 'Золотого', 'глобус', 'а', '»', ',', '«', 'Грэмми', '»', ',', '«', 'Тони', '»', 'и', '«', 'BA', 'F', 'TA', '»', '.', 'Ричард', 'Бёр', 'тон', 'родился', '10', 'ноября', '1925', 'года', 'в', 'деревне', 'По', 'нт', 'рид', 'иф', 'ен', '(', 'ныне', 'графство', 'Ни', 'т', '—', 'Порт', '-', 'Тол', 'бот', ')', ',', 'Уэль', 'с', '.', 'Сыграл', 'в', '11', 'фильмах', 'с', 'Тейлор', ',', 'среди', 'которых', 'знаменитый', 'фильм', 'Джозефа', 'Ман', 'кевича', '«', 'Кле', 'опа', 'тра', '»', ',', 'где', 'Барто', 'н', 'сыграл', 'роль', 'Марка', 'Антония', '.', 'Еще', 'один', 'знаменитый', 'фильм', ',', 'в', 'котором', 'сыграла', 'пара', '—', '«', 'Кто', 'боится', 'Вирджини', 'и', 'Вуль', 'ф', '»', 'Майка', 'Никол', 'са', '.', 'На', 'пике', 'своей', 'популярности', 'в', '1960', '-', 'е', 'годы', 'Бёр', 'тон', 'считался', 'одним', 'из', 'самых', 'высокооплачив', 'аемых', 'актёров', 'Голливуда', '.', 'В', 'общественном', 'сознании', 'его', 'до', 'сих', 'пор', 'тесно', 'связывают', 'с', 'его', 'второй', 'супругой', 'Элизабет', 'Тейлор', ',', 'бур', 'ные', 'отношения', 'с', 'которой', 'не', 'сходил', 'и', 'с', 'первых', 'полос', 'газет', '.', 'Барто', 'н', 'умер', 'в', '5', 'августа', '1984', 'года', 'в', 'возрасте', '58', 'лет', 'от', 'крово', 'из', 'лия', 'ния', 'в', 'мозг', 'в', 'своём', 'доме', 'в', 'муниципалитет', 'е', 'Се', 'лин', 'ьи', ',', 'Жене', 'ва', ',', 'Швейцарии', '.']]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'data/train'\n",
    "VALID_PATH = 'data/dev'\n",
    "TEST_PATH = 'data/test'\n",
    "\n",
    "train_token_seq, train_label_seq, train_spans_seq = read_data(TRAIN_PATH)\n",
    "\n",
    "test_token_seq, test_label_seq, test_spans_seq = read_data(TEST_PATH)\n",
    "\n",
    "valid_token_seq, valid_label_seq, valid_spans_seq = read_data(VALID_PATH)\n",
    "\n",
    "print(train_token_seq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcccc293-a942-432f-a2e7-76021402cd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-NATIONALITY', 'I-NATIONALITY', 'B-PROFESSION', 'O', 'B-PENALTY', 'I-PENALTY', 'I-PENALTY', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'B-PERSON', 'I-PERSON', 'B-NATIONALITY', 'I-NATIONALITY', 'B-PROFESSION', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PENALTY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-AGE', 'I-AGE', 'I-AGE', 'B-PROFESSION', 'O', 'B-CITY', 'I-CITY', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'B-PENALTY', 'I-PENALTY', 'I-PENALTY', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RELIGION', 'O', 'B-COUNTRY', 'I-COUNTRY', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'B-ORGANIZATION', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'B-NUMBER', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-EVENT', 'B-DATE', 'I-DATE', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'B-DATE', 'I-DATE', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-AGE', 'I-AGE', 'O', 'O', 'B-PROFESSION', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'B-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-COUNTRY', 'I-COUNTRY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O'], ['B-CITY', 'I-CITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-CITY', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'B-AGE', 'I-AGE', 'O', 'B-EVENT', 'O', 'O', 'B-DISTRICT', 'I-DISTRICT', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-EVENT', 'I-EVENT', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISTRICT', 'I-DISTRICT', 'I-DISTRICT', 'I-DISTRICT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'O', 'O', 'B-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'B-EVENT', 'O', 'B-DATE', 'I-DATE', 'O', 'B-ORDINAL', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-EVENT', 'O', 'B-EVENT', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AWARD', 'I-AWARD', 'I-AWARD', 'O', 'B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISTRICT', 'I-DISTRICT', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'O', 'B-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'B-EVENT', 'O', 'O', 'B-EVENT', 'I-EVENT', 'O'], ['B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'B-PERSON', 'O', 'O', 'B-EVENT', 'O', 'B-COUNTRY', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'O', 'B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-EVENT', 'O', 'B-COUNTRY', 'B-DATE', 'I-DATE', 'B-EVENT', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'B-AGE', 'I-AGE', 'I-AGE', 'B-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'B-NUMBER', 'I-NUMBER', 'O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-FAMILY', 'I-FAMILY', 'I-FAMILY', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'B-PERSON', 'I-PERSON', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-CITY', 'I-CITY', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'I-IDEOLOGY', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-AWARD', 'I-AWARD', 'I-AWARD', 'O'], ['O', 'B-ORGANIZATION', 'O', 'B-PENALTY', 'B-NATIONALITY', 'O', 'B-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'O', 'B-PENALTY', 'B-NATIONALITY', 'O', 'B-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'I-CRIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'O', 'O', 'B-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'O', 'O', 'O', 'O', 'B-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'I-PENALTY', 'O'], ['B-EVENT', 'I-EVENT', 'O', 'B-CITY', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'B-EVENT', 'B-PROFESSION', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'B-EVENT', 'O', 'B-TIME', 'B-NUMBER', 'O', 'O', 'O', 'B-EVENT', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'B-DISTRICT', 'I-DISTRICT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'B-PROFESSION', 'O', 'B-PROFESSION', 'I-PROFESSION', 'B-EVENT', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'B-NUMBER', 'O', 'B-COUNTRY', 'B-NATIONALITY', 'O', 'B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-AGE', 'O', 'B-AGE', 'O', 'B-AGE', 'I-AGE', 'O', 'O', 'O', 'O', 'B-IDEOLOGY', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'B-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-NATIONALITY', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-IDEOLOGY', 'I-IDEOLOGY', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'B-COUNTRY', 'I-COUNTRY', 'B-CITY', 'I-CITY', 'O', 'O', 'B-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'B-NATIONALITY', 'I-NATIONALITY', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'O'], ['B-EVENT', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'B-AGE', 'I-AGE', 'I-AGE', 'I-AGE', 'I-AGE', 'O', 'B-EVENT', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FACILITY', 'I-FACILITY', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'I-COUNTRY', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'B-PROFESSION', 'B-PERSON', 'I-PERSON', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-EVENT', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-EVENT', 'O', 'O', 'B-ORGANIZATION', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'B-ORGANIZATION', 'O', 'B-COUNTRY', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'B-PROFESSION', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'O', 'B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'O'], ['B-PERSON', 'B-EVENT', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'B-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'B-EVENT', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'B-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGE', 'I-AGE', 'I-AGE', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PROFESSION', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-COUNTRY', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-DATE', 'O', 'B-DATE', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-DATE', 'O', 'B-DATE', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O'], ['B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'B-EVENT', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-STATE_OR_PROVINCE', 'O', 'O', 'B-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'I-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-ORGANIZATION', 'O', 'B-EVENT', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'B-STATE_OR_PROVINCE', 'O', 'O', 'O', 'O', 'O', 'B-LAW', 'I-LAW', 'I-LAW', 'I-LAW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'B-CITY', 'I-CITY', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'B-PERCENT', 'I-PERCENT', 'I-PERCENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STATE_OR_PROVINCE', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'B-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-AWARD', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'B-DISTRICT', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PROFESSION', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-AWARD', 'O', 'B-FACILITY', 'I-FACILITY', 'I-FACILITY', 'O', 'B-CITY', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'B-EVENT', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-PERSON', 'I-PERSON', 'B-PROFESSION', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'B-NATIONALITY', 'B-PROFESSION', 'O', 'B-ORDINAL', 'I-ORDINAL', 'B-AWARD', 'I-AWARD', 'I-AWARD', 'I-AWARD', 'I-AWARD', 'I-AWARD', 'O', 'O', 'O', 'O', 'B-AWARD', 'I-AWARD', 'I-AWARD', 'O', 'O', 'O', 'B-AWARD', 'O', 'O', 'O', 'B-AWARD', 'O', 'O', 'O', 'B-AWARD', 'I-AWARD', 'I-AWARD', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'O', 'O', 'B-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'O', 'O', 'B-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'O', 'O', 'O', 'B-NUMBER', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROFESSION', 'B-DISTRICT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'B-EVENT', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-AGE', 'I-AGE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISTRICT', 'I-DISTRICT', 'I-DISTRICT', 'O', 'B-CITY', 'I-CITY', 'O', 'B-COUNTRY', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(train_label_seq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d2ac91-8750-4e46-a2d8-831e5ebef161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 6), (6, 11), (12, 20), (21, 26), (27, 34), (35, 37), (38, 44), (45, 47), (48, 60), (61, 62), (63, 68), (68, 69), (70, 77), (78, 83), (84, 90), (90, 94), (95, 102), (102, 103), (104, 108), (108, 115), (116, 117), (118, 130), (131, 134), (134, 137), (138, 143), (143, 144), (145, 146), (147, 154), (154, 155), (156, 157), (158, 163), (164, 168), (169, 173), (173, 174), (175, 183), (184, 187), (188, 195), (196, 197), (198, 205), (205, 212), (213, 216), (216, 217), (218, 221), (222, 227), (228, 239), (240, 247), (248, 249), (250, 253), (254, 264), (265, 266), (267, 272), (273, 274), (275, 289), (289, 290), (291, 293), (294, 295), (296, 306), (307, 312), (313, 326), (326, 327), (328, 335), (335, 339), (340, 346), (347, 348), (349, 354), (355, 369), (370, 379), (380, 381), (382, 392), (393, 395), (396, 411), (412, 418), (419, 427), (428, 434), (434, 435), (437, 439), (439, 442), (442, 445), (445, 447), (448, 451), (451, 453), (453, 455), (455, 456), (457, 459), (459, 460), (460, 466), (467, 474), (475, 477), (478, 481), (481, 483), (483, 484), (485, 493), (494, 499), (500, 507), (507, 514), (515, 520), (521, 522), (523, 526), (526, 527), (527, 536), (536, 537), (538, 543), (544, 548), (549, 552), (553, 555), (556, 563), (564, 566), (567, 572), (573, 581), (582, 583), (584, 592), (592, 593), (594, 597), (598, 603), (604, 609), (609, 612), (613, 618), (618, 624), (625, 629), (630, 632), (633, 641), (642, 648), (649, 650), (651, 657), (657, 658), (659, 662), (663, 666), (667, 672), (673, 683), (684, 687), (688, 690), (691, 694), (694, 695), (697, 710), (711, 718), (718, 725), (726, 733), (734, 743), (744, 747), (747, 749), (749, 752), (753, 754), (755, 759), (759, 767), (768, 776), (777, 783), (784, 787), (787, 789), (789, 790), (790, 791), (792, 795), (796, 798), (799, 809), (810, 811), (812, 814), (815, 822), (822, 823), (825, 828), (829, 836), (837, 843), (843, 847), (848, 855), (856, 859), (859, 861), (861, 864), (864, 865), (866, 876), (877, 885), (885, 886), (887, 890), (891, 894), (895, 902), (902, 906), (907, 909), (910, 917), (918, 924), (925, 932), (932, 933), (935, 940), (941, 945), (946, 949), (950, 951), (952, 958), (959, 966), (967, 970), (970, 972), (972, 974), (975, 986), (987, 991), (992, 1003), (1003, 1004), (1005, 1008), (1009, 1015), (1015, 1018), (1019, 1025), (1026, 1032), (1032, 1035), (1036, 1042), (1042, 1043), (1044, 1047), (1048, 1056), (1056, 1057), (1058, 1061), (1062, 1071), (1072, 1073), (1074, 1084), (1085, 1097), (1097, 1098), (1099, 1115), (1116, 1127), (1128, 1135), (1135, 1136), (1137, 1140), (1141, 1152), (1153, 1155), (1156, 1161), (1161, 1162), (1163, 1165), (1166, 1169), (1170, 1178), (1178, 1182), (1183, 1187), (1188, 1193), (1194, 1199), (1199, 1206), (1206, 1207), (1209, 1217), (1218, 1221), (1221, 1224), (1225, 1230), (1231, 1238), (1239, 1245), (1245, 1249), (1249, 1250), (1251, 1254), (1255, 1257), (1258, 1268), (1269, 1280), (1281, 1288), (1289, 1291), (1292, 1297), (1298, 1301), (1301, 1303), (1303, 1306), (1307, 1312), (1313, 1321), (1322, 1325), (1326, 1331), (1331, 1332), (1334, 1336), (1337, 1342), (1343, 1349), (1349, 1350), (1351, 1354), (1355, 1360), (1361, 1362), (1363, 1368), (1369, 1375), (1376, 1388), (1389, 1392), (1392, 1394), (1394, 1397), (1397, 1398), (1399, 1403), (1404, 1409), (1410, 1418), (1419, 1434), (1435, 1445), (1446, 1448), (1449, 1456), (1456, 1457), (1459, 1462), (1462, 1464), (1464, 1466), (1467, 1473), (1473, 1474), (1475, 1478), (1479, 1490), (1491, 1494), (1495, 1506), (1506, 1507), (1508, 1513), (1514, 1518), (1519, 1522), (1523, 1530), (1530, 1534), (1535, 1543), (1544, 1551), (1552, 1561), (1562, 1567), (1568, 1577), (1578, 1579), (1580, 1591), (1592, 1597), (1597, 1598), (1599, 1602), (1602, 1604), (1604, 1606), (1607, 1613), (1613, 1614), (1615, 1618), (1619, 1622), (1623, 1628), (1628, 1631), (1632, 1637), (1638, 1650), (1650, 1652), (1653, 1659), (1659, 1660), (1661, 1663), (1664, 1666), (1667, 1676), (1677, 1683), (1684, 1690), (1690, 1693), (1694, 1702), (1703, 1708), (1708, 1709)], [(0, 7), (8, 17), (18, 21), (21, 24), (24, 26), (27, 32), (33, 34), (35, 43), (46, 54), (55, 56), (57, 70), (71, 78), (78, 79), (80, 83), (84, 87), (87, 90), (90, 92), (93, 95), (96, 100), (100, 103), (104, 105), (106, 111), (112, 116), (116, 117), (117, 125), (125, 127), (127, 128), (129, 133), (134, 137), (137, 140), (140, 142), (144, 149), (150, 157), (157, 158), (158, 165), (166, 175), (176, 180), (181, 184), (184, 187), (187, 189), (189, 190), (191, 202), (203, 204), (205, 209), (209, 210), (211, 216), (217, 218), (219, 227), (227, 228), (229, 237), (238, 247), (248, 252), (252, 254), (255, 257), (258, 265), (266, 268), (269, 277), (278, 279), (280, 288), (288, 292), (293, 306), (306, 307), (309, 313), (314, 324), (324, 325), (326, 332), (333, 335), (336, 344), (345, 350), (351, 356), (356, 363), (364, 376), (377, 390), (391, 392), (392, 393), (393, 395), (395, 396), (397, 403), (404, 409), (409, 410), (412, 413), (413, 416), (416, 419), (419, 421), (422, 427), (428, 436), (437, 439), (440, 448), (449, 451), (451, 452), (452, 454), (455, 460), (460, 466), (467, 468), (469, 474), (475, 479), (479, 480), (480, 488), (488, 490), (490, 491), (491, 492), (493, 494), (495, 502), (503, 511), (512, 521), (522, 527), (528, 533), (533, 534), (536, 542), (543, 554), (554, 558), (559, 565), (566, 570), (571, 574), (574, 577), (577, 579), (580, 581), (582, 586), (587, 598), (599, 604), (605, 613), (613, 617), (618, 631), (632, 633), (634, 640), (641, 648), (648, 649), (650, 652), (653, 665), (666, 673), (674, 681), (682, 684), (685, 687), (687, 688), (689, 692), (693, 696), (697, 704), (705, 710), (710, 716), (716, 718), (719, 721), (722, 730), (731, 738), (739, 740), (741, 746), (746, 748), (749, 754), (755, 768), (769, 782), (783, 791), (791, 792), (793, 796), (797, 806), (807, 815), (816, 824), (825, 832), (832, 838), (839, 847), (848, 849), (849, 851), (851, 852), (853, 854), (855, 856), (856, 858), (858, 860), (860, 861), (863, 866), (866, 869), (869, 871), (871, 872), (873, 881), (882, 883), (884, 891), (892, 902), (903, 905), (906, 909), (909, 910), (911, 915), (916, 925), (926, 931), (932, 936), (936, 937), (938, 941), (942, 943), (944, 952), (953, 959), (960, 961), (962, 970), (971, 976), (977, 987), (988, 990), (991, 997), (997, 998), (999, 1001), (1002, 1009), (1009, 1013), (1014, 1016), (1017, 1023), (1024, 1031), (1031, 1032), (1032, 1039), (1040, 1043), (1043, 1045), (1046, 1047), (1047, 1049), (1049, 1050), (1050, 1051), (1053, 1054), (1054, 1056), (1056, 1058), (1059, 1060), (1061, 1066), (1066, 1070), (1071, 1082), (1083, 1085), (1086, 1093), (1094, 1100), (1101, 1115), (1116, 1121), (1121, 1122), (1123, 1130), (1131, 1142), (1142, 1143), (1144, 1152), (1153, 1158), (1159, 1160), (1161, 1164), (1164, 1166), (1167, 1168), (1169, 1176), (1177, 1191), (1192, 1201), (1202, 1209), (1210, 1218), (1218, 1223), (1224, 1230), (1231, 1239), (1239, 1240), (1241, 1248), (1249, 1262), (1262, 1263), (1264, 1269), (1269, 1277), (1277, 1285), (1286, 1297), (1297, 1298), (1300, 1305), (1305, 1312), (1313, 1320), (1321, 1331), (1332, 1342), (1343, 1351), (1352, 1362), (1363, 1364), (1365, 1375), (1376, 1389), (1390, 1396), (1397, 1398), (1399, 1407), (1408, 1417), (1418, 1424), (1425, 1429), (1430, 1436), (1436, 1437)], [(0, 7), (7, 10), (11, 15), (15, 19), (20, 22), (23, 32), (33, 36), (36, 37), (38, 42), (42, 43), (43, 48), (49, 51), (52, 59), (60, 64), (65, 69), (70, 71), (72, 80), (81, 89), (90, 94), (95, 106), (107, 113), (113, 114), (115, 123), (124, 135), (136, 138), (139, 142), (142, 143), (145, 153), (154, 157), (158, 165), (166, 170), (170, 174), (175, 187), (188, 192), (193, 194), (194, 199), (199, 202), (202, 203), (204, 207), (208, 217), (218, 231), (232, 243), (244, 250), (250, 251), (253, 257), (257, 262), (263, 271), (272, 273), (274, 280), (281, 289), (290, 291), (291, 293), (293, 294), (294, 300), (300, 301), (301, 302), (303, 308), (309, 311), (311, 317), (318, 320), (321, 326), (326, 327), (328, 331), (332, 334), (334, 336), (336, 338), (338, 341), (342, 347), (347, 352), (353, 365), (366, 376), (377, 379), (380, 389), (390, 393), (393, 394), (394, 395), (396, 401), (401, 404), (405, 410), (411, 420), (421, 423), (424, 430), (431, 439), (440, 446), (447, 448), (449, 461), (462, 469), (469, 471), (472, 479), (480, 486), (486, 487), (488, 491), (492, 497), (498, 501), (502, 510), (511, 516), (517, 524), (525, 532), (533, 535), (536, 546), (547, 552), (552, 556), (557, 558), (559, 565), (565, 572), (573, 585), (585, 586), (587, 590), (591, 599), (600, 604), (605, 617), (618, 621), (621, 625), (625, 626), (627, 628), (629, 634), (635, 638), (639, 644), (644, 646), (647, 654), (654, 655), (657, 670), (671, 682), (683, 690), (690, 693), (694, 703), (704, 705), (706, 713), (714, 721), (722, 729), (730, 735), (735, 739), (740, 748), (749, 757), (758, 759), (759, 761), (761, 762), (762, 768), (768, 769), (769, 770), (771, 776), (776, 785), (786, 794), (795, 799), (799, 801), (801, 803), (804, 811), (812, 818), (819, 822), (823, 827), (827, 828), (829, 835), (836, 841), (841, 846), (847, 851), (852, 860), (861, 862), (863, 871), (872, 876), (877, 879), (880, 886), (887, 891), (891, 892), (893, 902), (903, 904), (905, 910), (911, 912), (913, 914), (915, 923), (924, 925), (926, 933), (934, 938), (938, 939), (940, 941), (942, 945), (946, 955), (956, 963), (964, 967), (968, 976), (977, 986), (986, 987), (988, 996), (996, 999), (1000, 1012), (1013, 1015), (1016, 1019), (1020, 1024), (1025, 1026), (1027, 1034), (1035, 1037), (1038, 1044), (1045, 1050), (1050, 1055), (1056, 1062), (1063, 1064), (1065, 1070), (1071, 1076), (1076, 1077), (1079, 1084), (1084, 1089), (1090, 1095), (1096, 1105), (1106, 1107), (1108, 1114), (1115, 1116), (1117, 1130), (1131, 1143), (1144, 1148), (1149, 1160), (1161, 1162), (1163, 1173), (1174, 1179), (1180, 1191), (1192, 1200), (1201, 1202), (1203, 1209), (1210, 1215), (1215, 1216), (1217, 1222), (1222, 1223), (1224, 1229), (1229, 1232), (1233, 1241), (1242, 1247), (1247, 1248), (1249, 1258), (1259, 1266), (1267, 1272), (1272, 1274), (1274, 1275), (1276, 1283), (1283, 1285), (1286, 1288), (1289, 1293), (1293, 1294), (1294, 1297), (1297, 1298), (1299, 1306), (1306, 1308), (1309, 1311), (1312, 1318), (1319, 1324), (1324, 1325), (1326, 1329), (1329, 1332), (1332, 1334), (1334, 1335), (1336, 1347), (1348, 1351), (1351, 1355), (1356, 1357), (1358, 1369), (1369, 1371), (1372, 1374), (1375, 1386), (1387, 1395), (1396, 1401), (1401, 1402), (1404, 1408), (1408, 1409), (1409, 1412), (1413, 1421), (1422, 1430), (1430, 1432), (1433, 1441), (1442, 1447), (1447, 1449), (1449, 1452), (1452, 1453), (1454, 1455), (1456, 1461), (1462, 1465), (1466, 1469), (1470, 1476), (1476, 1477), (1478, 1479), (1480, 1482), (1483, 1492), (1493, 1496), (1497, 1510), (1511, 1517), (1517, 1518), (1519, 1520), (1520, 1524), (1525, 1532), (1533, 1538), (1539, 1544), (1545, 1548), (1549, 1554), (1555, 1559), (1559, 1563), (1563, 1564), (1565, 1566), (1567, 1579), (1580, 1587), (1588, 1592), (1592, 1594), (1594, 1595), (1596, 1597), (1598, 1599), (1600, 1607), (1608, 1621), (1622, 1623), (1624, 1625), (1626, 1631), (1632, 1633), (1633, 1638), (1638, 1641), (1641, 1642), (1643, 1648), (1649, 1655), (1656, 1664), (1664, 1665), (1666, 1669), (1670, 1671), (1672, 1680), (1681, 1689), (1690, 1700), (1701, 1709), (1710, 1719), (1720, 1732), (1733, 1740), (1741, 1744), (1745, 1749), (1749, 1750), (1751, 1755), (1756, 1760), (1761, 1766), (1767, 1776), (1777, 1778), (1779, 1783), (1784, 1785), (1786, 1791), (1792, 1800), (1801, 1809), (1809, 1810), (1811, 1818), (1819, 1821), (1822, 1829), (1830, 1840), (1841, 1846), (1847, 1856), (1857, 1866), (1867, 1870), (1871, 1877), (1877, 1885), (1885, 1886), (1886, 1887), (1888, 1889), (1890, 1896), (1897, 1904), (1904, 1905), (1907, 1914), (1915, 1918), (1919, 1927), (1928, 1936), (1937, 1943), (1944, 1945), (1946, 1954), (1955, 1960), (1961, 1970), (1970, 1971), (1972, 1978), (1979, 1988), (1989, 1992), (1993, 1995), (1996, 2006), (2007, 2013), (2014, 2017), (2018, 2019), (2020, 2021), (2022, 2026), (2026, 2027), (2029, 2037), (2037, 2039), (2040, 2048), (2049, 2058), (2058, 2061), (2062, 2070), (2070, 2073), (2073, 2074)], [(0, 5), (6, 11), (12, 20), (21, 27), (27, 28), (29, 30), (31, 38), (39, 46), (47, 51), (52, 56), (56, 59), (62, 67), (68, 77), (78, 79), (80, 87), (88, 89), (90, 93), (93, 94), (95, 104), (104, 105), (106, 109), (110, 119), (120, 123), (124, 131), (132, 134), (135, 142), (143, 146), (146, 149), (149, 151), (151, 155), (156, 162), (162, 163), (164, 165), (166, 173), (174, 183), (184, 185), (186, 196), (197, 201), (201, 204), (204, 205), (206, 207), (208, 212), (213, 221), (222, 223), (224, 230), (230, 234), (234, 238), (239, 247), (247, 248), (251, 260), (261, 264), (265, 270), (271, 276), (277, 278), (279, 285), (286, 292), (293, 294), (295, 298), (299, 300), (301, 312), (313, 320), (321, 327), (328, 331), (331, 334), (334, 336), (336, 340), (340, 341), (342, 345), (346, 357), (358, 364), (364, 365), (366, 367), (368, 375), (376, 382), (383, 387), (388, 397), (398, 399), (400, 410), (411, 416), (417, 426), (427, 428), (429, 433), (434, 439), (440, 446), (447, 449), (449, 452), (452, 454), (454, 456), (457, 464), (465, 469), (469, 472), (472, 473), (474, 482), (483, 492), (493, 498), (499, 504), (504, 505), (507, 510), (511, 520), (520, 521), (522, 527), (528, 535), (536, 538), (539, 546), (547, 553), (554, 555), (556, 562), (563, 569), (570, 574), (574, 577), (578, 579), (580, 584), (585, 593), (594, 595), (596, 602), (602, 606), (606, 610), (611, 613), (613, 614), (614, 621), (622, 630), (630, 631), (632, 635), (635, 640), (641, 647), (648, 650), (651, 654), (654, 657), (657, 659), (659, 663), (663, 664), (665, 668), (669, 670), (671, 675), (676, 678), (679, 683), (684, 687), (688, 696), (696, 698), (699, 711), (712, 718), (719, 726), (726, 729), (730, 739), (740, 743), (743, 744), (745, 754), (755, 756), (757, 759), (760, 770), (771, 773), (774, 777), (777, 780), (780, 783), (783, 784), (786, 794), (795, 800), (801, 810), (811, 812), (813, 819), (820, 824), (824, 827), (827, 828), (829, 832), (833, 837), (837, 840), (841, 850), (851, 852), (853, 864), (865, 874), (874, 875), (876, 877), (878, 883), (884, 891), (892, 897), (897, 898), (899, 902), (903, 905), (906, 912), (913, 914), (915, 926), (927, 933), (934, 941), (942, 943), (944, 947), (947, 948), (950, 951), (952, 958), (959, 963), (964, 968), (968, 971), (972, 977), (978, 979), (980, 989), (990, 991), (992, 996), (996, 1000), (1001, 1002), (1003, 1009), (1009, 1012), (1013, 1017), (1017, 1021), (1022, 1030), (1030, 1031), (1032, 1041), (1042, 1046), (1047, 1052), (1053, 1056), (1057, 1066), (1067, 1072), (1073, 1084), (1084, 1085), (1086, 1090), (1091, 1096), (1097, 1102), (1103, 1113), (1113, 1114), (1115, 1116), (1117, 1124), (1125, 1134), (1135, 1138), (1139, 1146), (1147, 1149), (1149, 1151), (1152, 1159), (1159, 1160), (1161, 1164), (1165, 1169), (1169, 1172), (1173, 1175), (1175, 1176), (1176, 1184), (1185, 1194), (1195, 1196), (1197, 1208), (1209, 1218), (1218, 1219), (1220, 1222), (1223, 1229), (1230, 1238), (1238, 1239), (1240, 1243), (1244, 1247), (1248, 1254), (1255, 1260), (1261, 1262), (1263, 1266), (1267, 1272), (1273, 1278), (1279, 1282), (1282, 1284), (1284, 1286), (1286, 1287), (1289, 1298), (1299, 1304), (1305, 1311), (1312, 1318), (1319, 1321), (1321, 1324), (1324, 1326), (1326, 1328), (1328, 1329), (1330, 1334), (1334, 1337), (1338, 1344), (1345, 1346), (1347, 1353), (1354, 1356), (1357, 1360), (1360, 1361), (1362, 1363), (1364, 1367), (1368, 1372), (1373, 1377), (1378, 1380), (1381, 1385), (1386, 1392), (1393, 1400), (1400, 1403), (1404, 1415), (1416, 1419), (1420, 1421), (1422, 1431), (1432, 1444), (1445, 1447), (1448, 1452), (1453, 1457), (1458, 1462), (1462, 1463), (1464, 1465), (1466, 1470), (1471, 1475), (1476, 1479), (1480, 1489), (1490, 1501), (1502, 1509), (1510, 1514), (1514, 1515)], [(0, 1), (2, 18), (19, 24), (25, 35), (36, 43), (44, 46), (47, 55), (56, 57), (58, 67), (68, 72), (72, 79), (80, 89), (91, 94), (94, 96), (96, 99), (99, 100), (101, 107), (107, 112), (113, 115), (116, 121), (121, 123), (123, 125), (126, 127), (128, 144), (145, 155), (156, 166), (167, 174), (175, 177), (178, 186), (187, 188), (189, 198), (199, 203), (203, 210), (211, 220), (220, 221), (223, 233), (233, 236), (237, 247), (248, 256), (257, 258), (259, 265), (266, 268), (269, 271), (272, 288), (289, 304), (305, 306), (306, 307), (308, 314), (315, 316), (317, 327), (327, 328), (329, 337), (338, 339), (340, 349), (349, 357), (358, 366), (366, 369), (370, 371), (372, 376), (376, 377), (378, 379), (380, 382), (383, 390), (390, 391), (392, 404), (405, 417), (418, 427), (428, 429), (430, 434), (435, 441), (442, 443), (444, 451), (452, 455), (456, 461), (462, 468), (468, 469), (470, 485), (486, 492), (493, 505), (506, 512), (513, 524), (525, 527), (528, 535), (536, 544), (545, 556), (557, 562), (562, 563), (565, 571), (571, 572), (573, 583), (584, 594), (595, 597), (598, 603), (604, 606), (607, 608), (609, 611), (612, 613), (614, 617), (617, 618), (619, 622), (622, 623), (623, 624), (625, 636), (637, 640), (641, 642), (643, 657), (658, 674), (675, 676), (677, 679), (680, 682), (683, 686), (686, 687), (688, 690), (691, 693), (694, 697), (697, 698), (699, 702), (702, 703), (703, 704), (705, 716), (717, 720), (721, 722), (723, 725), (726, 728), (729, 732), (732, 733), (734, 736), (737, 740), (741, 744), (744, 745), (746, 749), (749, 750), (752, 754), (755, 759), (760, 767), (768, 785), (786, 791), (792, 803), (804, 812), (812, 815), (816, 820), (820, 827), (828, 837), (837, 838)], [(0, 4), (4, 6), (7, 8), (9, 19), (19, 20), (21, 26), (27, 31), (32, 43), (44, 49), (49, 52), (53, 54), (55, 65), (66, 73), (74, 85), (86, 87), (88, 95), (95, 96), (97, 99), (100, 104), (105, 109), (110, 114), (114, 115), (116, 117), (118, 127), (128, 138), (139, 144), (145, 146), (147, 151), (151, 155), (156, 161), (162, 171), (172, 178), (178, 179), (181, 186), (187, 191), (192, 203), (204, 214), (215, 221), (222, 230), (231, 236), (237, 239), (239, 243), (243, 244), (245, 250), (251, 258), (259, 265), (265, 266), (267, 272), (273, 277), (278, 288), (289, 296), (297, 298), (299, 307), (308, 309), (310, 315), (316, 318), (319, 322), (322, 326), (327, 329), (330, 334), (334, 338), (339, 343), (343, 344), (346, 350), (350, 353), (354, 365), (366, 367), (368, 375), (375, 380), (381, 398), (399, 400), (401, 409), (410, 413), (413, 415), (416, 419), (419, 420), (420, 422), (422, 423), (423, 424), (424, 427), (427, 428), (429, 430), (430, 433), (434, 440), (441, 445), (446, 448), (449, 455), (455, 456), (456, 457), (458, 464), (465, 469), (470, 472), (473, 476), (477, 478), (479, 486), (487, 493), (493, 494), (496, 501), (502, 511), (512, 516), (517, 519), (520, 524), (524, 528), (529, 533), (534, 537), (538, 544), (544, 545), (546, 547), (548, 551), (552, 562), (563, 564), (565, 577), (577, 578), (580, 585), (586, 588), (588, 589), (589, 591), (592, 601), (602, 611), (612, 613), (614, 617), (617, 618), (619, 622), (623, 628), (628, 634), (635, 644), (645, 647), (648, 652), (652, 656), (657, 661), (662, 671), (672, 676), (677, 684), (685, 697), (698, 707), (708, 714), (715, 717), (717, 718), (719, 722), (722, 723), (723, 726), (726, 727), (727, 728), (730, 733), (734, 738), (738, 739), (740, 742), (743, 753), (753, 754), (755, 761), (762, 766), (767, 770), (771, 772), (773, 781), (782, 784), (784, 787), (787, 790), (790, 791), (792, 794), (795, 799), (800, 802), (802, 803), (804, 806), (807, 808), (809, 811), (812, 815), (815, 816), (817, 820), (820, 823), (824, 829), (830, 841), (841, 842), (843, 851), (852, 857), (858, 866), (867, 869), (869, 872), (872, 875), (875, 876), (877, 885), (886, 890), (890, 891), (892, 894), (894, 895), (896, 899), (899, 900), (900, 902), (902, 904), (904, 905), (906, 908), (908, 911), (911, 914), (915, 916), (917, 925), (926, 931), (932, 934), (934, 938), (939, 941), (941, 944), (944, 947), (947, 948), (950, 954), (955, 957), (958, 970), (971, 973), (974, 978), (979, 981), (982, 989), (990, 992), (993, 997), (998, 1010), (1011, 1016), (1017, 1018), (1019, 1027), (1028, 1029), (1030, 1038), (1038, 1039), (1040, 1041), (1041, 1044), (1044, 1047), (1047, 1048), (1049, 1055), (1056, 1061), (1062, 1067), (1067, 1068), (1068, 1069), (1070, 1073), (1074, 1084), (1085, 1090), (1091, 1103), (1104, 1110), (1111, 1112), (1112, 1117), (1117, 1124), (1125, 1139), (1140, 1145), (1145, 1146), (1146, 1147), (1148, 1158), (1158, 1162), (1163, 1164), (1164, 1166), (1166, 1168), (1168, 1169), (1171, 1172), (1172, 1174), (1174, 1176), (1177, 1179), (1180, 1185), (1186, 1188), (1189, 1193), (1194, 1209), (1210, 1212), (1213, 1217), (1218, 1224), (1224, 1225), (1226, 1235), (1235, 1236), (1237, 1240), (1241, 1245), (1246, 1256), (1257, 1261), (1262, 1270), (1270, 1273), (1274, 1283), (1284, 1289), (1290, 1291), (1291, 1296), (1296, 1301), (1302, 1310), (1310, 1311), (1312, 1313), (1314, 1315), (1315, 1320), (1321, 1327), (1328, 1330), (1331, 1332), (1332, 1338), (1338, 1343), (1344, 1350), (1351, 1354), (1354, 1355), (1355, 1357), (1357, 1359), (1359, 1360), (1360, 1361), (1363, 1364), (1365, 1370), (1371, 1372), (1373, 1376), (1376, 1377), (1378, 1381), (1382, 1392), (1392, 1393), (1394, 1401), (1402, 1406), (1406, 1410), (1411, 1413), (1414, 1418), (1418, 1422), (1423, 1427), (1428, 1436), (1436, 1438), (1439, 1440), (1441, 1455), (1456, 1462), (1462, 1463), (1464, 1473), (1474, 1481), (1482, 1491), (1491, 1496), (1497, 1503), (1504, 1507), (1507, 1509), (1510, 1512), (1512, 1513), (1513, 1517), (1518, 1530), (1531, 1544), (1545, 1555), (1556, 1558), (1559, 1564), (1564, 1565), (1566, 1568), (1569, 1573), (1573, 1577), (1578, 1582), (1583, 1588), (1589, 1595), (1595, 1600), (1600, 1605), (1605, 1606), (1606, 1609), (1609, 1612), (1613, 1615), (1616, 1618), (1619, 1622), (1623, 1624), (1625, 1630), (1631, 1633), (1634, 1638), (1638, 1642), (1643, 1647), (1648, 1659), (1660, 1669), (1670, 1676), (1676, 1677), (1679, 1680), (1681, 1685), (1686, 1687), (1688, 1690), (1691, 1693), (1694, 1696), (1697, 1701), (1702, 1703), (1704, 1711), (1712, 1718), (1718, 1720), (1721, 1723), (1723, 1727), (1728, 1734), (1735, 1749), (1750, 1762), (1763, 1764), (1765, 1777), (1778, 1787), (1788, 1794), (1795, 1802), (1803, 1804), (1805, 1813), (1814, 1816), (1816, 1817), (1817, 1819), (1820, 1828), (1829, 1831), (1832, 1836), (1836, 1840), (1841, 1845), (1845, 1846), (1847, 1853), (1853, 1855), (1856, 1863), (1864, 1873), (1874, 1880), (1881, 1891), (1892, 1893), (1894, 1902), (1902, 1903)], [(0, 9), (10, 16), (17, 24), (24, 25), (25, 32), (33, 39), (40, 46), (47, 53), (53, 55), (55, 58), (61, 62), (63, 67), (68, 70), (71, 72), (72, 73), (73, 75), (76, 78), (79, 80), (80, 81), (81, 82), (83, 89), (90, 92), (93, 95), (95, 96), (96, 97), (98, 102), (103, 108), (109, 118), (119, 122), (122, 123), (123, 130), (130, 131), (131, 138), (139, 145), (146, 152), (153, 159), (159, 161), (161, 164), (164, 165), (166, 172), (173, 179), (179, 181), (181, 184), (184, 185), (186, 189), (190, 199), (199, 200), (201, 206), (207, 216), (217, 219), (220, 232), (233, 241), (241, 242), (244, 250), (251, 257), (257, 259), (259, 262), (263, 269), (270, 286), (287, 294), (295, 296), (297, 304), (305, 319), (320, 330), (331, 336), (336, 337), (338, 339), (340, 345), (346, 349), (349, 351), (351, 354), (354, 358), (359, 370), (371, 372), (372, 376), (377, 378), (378, 379), (379, 380), (381, 383), (384, 388), (389, 402), (403, 412), (413, 429), (430, 438), (439, 447), (448, 449), (449, 456), (456, 457), (457, 458), (459, 460), (461, 467), (468, 481), (482, 488), (489, 496), (497, 498), (498, 505), (506, 510), (511, 512), (512, 513), (513, 514), (515, 521), (521, 523), (523, 526), (527, 530), (531, 539), (540, 553), (554, 560), (561, 570), (571, 573), (573, 574), (575, 579), (580, 591), (592, 600), (601, 603), (604, 611), (612, 624), (625, 627), (628, 632), (633, 637), (637, 638), (638, 640), (641, 645), (645, 646), (648, 649), (650, 657), (657, 658), (658, 666), (667, 671), (672, 673), (673, 674), (674, 675), (676, 678), (679, 684), (685, 703), (704, 711), (711, 712), (713, 719), (719, 721), (721, 724), (725, 730), (731, 739), (740, 751), (752, 764), (765, 778), (779, 781), (781, 782), (783, 788), (789, 794), (795, 802), (802, 803), (804, 807), (808, 809), (810, 814), (814, 815), (815, 816), (816, 817), (818, 824), (824, 826), (826, 829), (830, 835), (836, 845), (846, 853), (854, 855), (855, 862), (862, 863), (863, 864), (866, 867), (868, 874), (875, 879), (879, 880), (880, 882), (882, 883), (884, 889), (890, 896), (897, 904), (905, 913), (914, 918), (919, 925), (926, 935), (935, 936), (937, 943), (944, 950), (950, 952), (952, 955), (956, 959), (960, 968), (969, 980), (981, 995), (996, 1006), (1007, 1009), (1010, 1012), (1013, 1027), (1028, 1037), (1038, 1040), (1041, 1046), (1046, 1049), (1049, 1050), (1052, 1053), (1054, 1062), (1063, 1067), (1068, 1069), (1069, 1070), (1071, 1074), (1074, 1075), (1075, 1082), (1083, 1087), (1088, 1099), (1099, 1103), (1104, 1114), (1114, 1119), (1120, 1126), (1127, 1133), (1133, 1136), (1136, 1137), (1137, 1150), (1151, 1157), (1158, 1166), (1167, 1181), (1181, 1182), (1184, 1185), (1186, 1190), (1190, 1191), (1191, 1195), (1196, 1198), (1198, 1199), (1200, 1206), (1207, 1213), (1213, 1215), (1215, 1218), (1219, 1226), (1227, 1233), (1234, 1236), (1237, 1238), (1239, 1246), (1246, 1247), (1248, 1259), (1260, 1274), (1275, 1285), (1286, 1288), (1289, 1291), (1292, 1299), (1299, 1300), (1300, 1313), (1314, 1320), (1321, 1326), (1327, 1332), (1333, 1341), (1343, 1346), (1347, 1355), (1356, 1363), (1364, 1369), (1369, 1370), (1370, 1376), (1377, 1383), (1383, 1384), (1385, 1394), (1395, 1401), (1402, 1409), (1410, 1418), (1419, 1426), (1427, 1435), (1436, 1445), (1446, 1460), (1461, 1467), (1468, 1469), (1470, 1477), (1478, 1482), (1482, 1491), (1491, 1495), (1496, 1501), (1502, 1513), (1514, 1516), (1517, 1525), (1526, 1540), (1541, 1555), (1556, 1557), (1558, 1571), (1571, 1572), (1572, 1583), (1584, 1587), (1588, 1595), (1596, 1602), (1602, 1604), (1604, 1608), (1609, 1610), (1611, 1616), (1617, 1618), (1619, 1622), (1623, 1626), (1626, 1631), (1631, 1632), (1634, 1638), (1639, 1647), (1648, 1662), (1663, 1664), (1665, 1670), (1671, 1672), (1673, 1676), (1676, 1681), (1682, 1689), (1690, 1696), (1696, 1698), (1698, 1702), (1703, 1708), (1709, 1716), (1717, 1724), (1725, 1733), (1734, 1739), (1740, 1751), (1752, 1765), (1766, 1774), (1775, 1780), (1780, 1781)], [(0, 5), (6, 14), (15, 17), (17, 19), (19, 23), (24, 26), (27, 33), (34, 46), (47, 52), (53, 55), (55, 57), (57, 60), (61, 70), (71, 77), (78, 86), (87, 92), (93, 101), (102, 108), (109, 111), (111, 113), (113, 117), (118, 120), (121, 128), (129, 135), (136, 148), (149, 155), (155, 156), (157, 172), (173, 177), (178, 189), (190, 197), (198, 200), (201, 212), (213, 221), (221, 222), (222, 229), (230, 238), (239, 249), (249, 250), (251, 254), (254, 257), (258, 259), (260, 266), (267, 273), (274, 286), (287, 297), (298, 307), (307, 308), (309, 318), (318, 321), (322, 328), (329, 339), (340, 350), (351, 360), (361, 363), (364, 366), (367, 370), (371, 375), (376, 380), (380, 381), (382, 391), (391, 392), (393, 399), (399, 401), (402, 404), (405, 409), (410, 412), (412, 414), (414, 418), (419, 425), (426, 434), (434, 439), (441, 442), (443, 449), (449, 450), (451, 452), (453, 461), (461, 465), (466, 473), (474, 476), (477, 487), (487, 488), (490, 492), (492, 493), (493, 499), (500, 505), (506, 508), (508, 510), (510, 513), (514, 515), (516, 524), (525, 527), (528, 529), (529, 536), (536, 537), (538, 544), (544, 545), (546, 556), (557, 563), (564, 573), (574, 575), (576, 582), (583, 584), (585, 592), (593, 603), (604, 607), (608, 609), (609, 613), (613, 614), (614, 618), (618, 619), (619, 620), (621, 633), (634, 649), (650, 654), (655, 665), (666, 667), (668, 674), (675, 682), (683, 684), (684, 688), (688, 689), (689, 693), (693, 694), (694, 695), (696, 708), (709, 716), (717, 723), (724, 730), (731, 732), (732, 738), (739, 745), (745, 746), (747, 748), (748, 749), (750, 754), (755, 759), (759, 760), (760, 761), (762, 768), (768, 773), (774, 787), (788, 794), (795, 796), (797, 803), (803, 807), (808, 814), (815, 817), (818, 832), (833, 841), (842, 843), (844, 853), (854, 859), (860, 867), (867, 868)], [(0, 13), (14, 23), (24, 34), (35, 36), (37, 45), (47, 51), (51, 53), (54, 57), (57, 62), (63, 72), (72, 76), (77, 81), (82, 91), (92, 94), (95, 108), (109, 121), (122, 130), (131, 140), (141, 148), (149, 153), (153, 156), (157, 160), (160, 162), (163, 171), (172, 176), (176, 179), (180, 183), (183, 188), (188, 189), (190, 191), (192, 195), (196, 209), (210, 211), (212, 220), (220, 221), (222, 224), (224, 227), (227, 231), (232, 238), (239, 245), (246, 251), (252, 259), (260, 266), (267, 277), (278, 279), (280, 287), (288, 296), (296, 297), (299, 304), (305, 312), (313, 323), (324, 330), (331, 340), (341, 344), (345, 358), (359, 360), (360, 361), (362, 370), (371, 374), (375, 381), (382, 393), (394, 401), (401, 402), (403, 406), (407, 415), (416, 420), (421, 428), (429, 435), (436, 446), (446, 447), (448, 451), (452, 459), (460, 464), (465, 472), (473, 475), (476, 485), (486, 494), (495, 504), (505, 506), (507, 514), (514, 515), (516, 518), (519, 526), (526, 527), (529, 534), (535, 539), (540, 545), (546, 558), (559, 565), (565, 566), (567, 569), (570, 577), (577, 581), (582, 592), (592, 593), (594, 596), (597, 604), (605, 610), (611, 613), (613, 614), (615, 626), (626, 630), (631, 642), (643, 645), (646, 659), (660, 667), (667, 668), (669, 674), (674, 678), (679, 685), (686, 698), (699, 707), (708, 709), (710, 716), (717, 723), (724, 725), (725, 732), (732, 736), (737, 743), (743, 744), (744, 745), (746, 748), (749, 755), (756, 758), (759, 764), (765, 772), (773, 779), (779, 780), (782, 785), (786, 790), (791, 794), (794, 796), (797, 804), (804, 805), (806, 809), (810, 813), (814, 816), (817, 825), (826, 835), (835, 836), (837, 839), (840, 847), (847, 849), (850, 859), (859, 860), (861, 865), (866, 870), (871, 874), (874, 879), (879, 880), (881, 894), (894, 895), (896, 901), (901, 903), (903, 906), (907, 913), (914, 920), (920, 921)], [(0, 6), (7, 10), (10, 13), (14, 21), (22, 26), (27, 33), (34, 36), (37, 39), (39, 42), (43, 48), (49, 50), (51, 60), (61, 67), (68, 71), (71, 74), (75, 81), (81, 83), (84, 93), (94, 99), (100, 106), (107, 110), (110, 113), (114, 121), (122, 126), (127, 133), (134, 136), (137, 139), (139, 142), (143, 148), (149, 150), (151, 160), (160, 161), (162, 167), (168, 169), (170, 178), (179, 185), (185, 186), (187, 189), (190, 197), (198, 201), (202, 207), (208, 214), (214, 215), (217, 219), (220, 229), (230, 244), (245, 248), (249, 253), (254, 259), (260, 261), (262, 266), (267, 272), (272, 274), (275, 280), (281, 286), (287, 290), (290, 291), (293, 299), (300, 303), (303, 306), (307, 308), (308, 315), (316, 318), (318, 322), (322, 323), (324, 325), (326, 336), (337, 342), (342, 343), (344, 348), (348, 355), (356, 363), (363, 364), (365, 367), (368, 374), (375, 376), (376, 381), (381, 382), (382, 383), (384, 394), (395, 396), (396, 404), (405, 411), (411, 412), (412, 413), (413, 414), (415, 416), (416, 422), (422, 423), (423, 424), (425, 426), (426, 430), (430, 431), (432, 433), (434, 435), (435, 437), (437, 438), (438, 440), (440, 441), (441, 442), (444, 450), (451, 454), (454, 457), (458, 465), (466, 468), (469, 475), (476, 480), (481, 485), (486, 487), (488, 495), (496, 498), (498, 500), (500, 503), (503, 505), (505, 507), (508, 509), (509, 513), (514, 522), (523, 525), (525, 526), (527, 528), (529, 533), (533, 534), (534, 537), (537, 540), (540, 541), (541, 542), (543, 547), (547, 548), (548, 549), (551, 557), (558, 559), (560, 562), (563, 570), (571, 572), (573, 579), (579, 580), (581, 586), (587, 594), (595, 605), (606, 611), (612, 619), (620, 623), (623, 629), (630, 631), (631, 634), (634, 637), (637, 640), (640, 641), (641, 642), (643, 646), (647, 652), (652, 653), (654, 660), (661, 665), (666, 671), (672, 679), (679, 680), (681, 684), (685, 689), (690, 700), (701, 706), (706, 707), (708, 709), (710, 717), (718, 725), (726, 730), (731, 732), (733, 734), (734, 737), (738, 744), (745, 753), (753, 754), (755, 759), (759, 760), (760, 761), (762, 767), (768, 773), (773, 775), (775, 776), (778, 780), (781, 785), (786, 791), (792, 804), (805, 806), (807, 811), (811, 812), (812, 813), (814, 818), (819, 822), (822, 825), (826, 834), (835, 840), (841, 843), (844, 849), (850, 863), (863, 868), (869, 876), (877, 886), (886, 887), (888, 889), (890, 902), (903, 911), (912, 915), (916, 918), (919, 922), (923, 926), (927, 932), (933, 942), (943, 944), (945, 948), (949, 955), (956, 964), (965, 973), (974, 980), (980, 981), (982, 985), (985, 988), (989, 998), (999, 1000), (1001, 1008), (1009, 1011), (1012, 1018), (1018, 1019), (1020, 1021), (1022, 1028), (1029, 1034), (1035, 1040), (1040, 1041), (1043, 1048), (1048, 1049), (1050, 1054), (1055, 1056), (1057, 1058), (1059, 1066), (1067, 1071), (1072, 1076), (1077, 1078), (1079, 1087), (1088, 1090), (1091, 1094), (1095, 1097), (1098, 1103), (1103, 1105), (1105, 1108), (1108, 1111), (1112, 1113), (1114, 1118), (1119, 1120), (1121, 1126), (1127, 1131), (1132, 1133), (1134, 1147), (1147, 1148), (1149, 1151), (1151, 1154), (1154, 1156), (1156, 1157), (1158, 1162), (1162, 1164), (1164, 1165), (1166, 1175), (1175, 1176)]]\n"
     ]
    }
   ],
   "source": [
    "print(train_spans_seq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b3b5f-9578-4223-93d6-d2e89d802448",
   "metadata": {},
   "source": [
    "## Создание Dataset и Collator (ЧАСТЬ ВЗЯТА ИЗ НОУТБУКА ПО DL \"Домашнее задание 3 Named Entity Recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbd854d-f14f-4ddf-b75d-98f51a3600be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b89c101-76d4-4e67-909e-9d8b84b075c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# используйте параметр min_count для того, чтобы отсекать слова частотой cnt < min_count\n",
    "\n",
    "def get_token2idx(\n",
    "    token2cnt: Dict[str, int],\n",
    "    min_count: int,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    token2idx: Dict[str, int] = {}\n",
    "    token2idx[\"<PAD>\"], token2idx[\"<UNK>\"] = 0, 1\n",
    "    idx = 0\n",
    "    for token, cnt in token2cnt.items():\n",
    "        if cnt < min_count:\n",
    "            continue\n",
    "        idx += 1\n",
    "        token2idx[token] = idx + 1 # потому что 0 1 индексы уже используем\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e404e3cc-6e99-4d87-afb9-6ed65888df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = get_token2idx(token2cnt, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf69b284-1f92-452a-9a3d-b6b1d889e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сортировки тегов, чтобы сначала был тег O, потом теги B- и только после теги I- (можно задать вручную)\n",
    "\n",
    "def sort_labels_func(x: str) -> int:\n",
    "    if x == \"O\":\n",
    "        return 0\n",
    "    elif x.startswith(\"B-\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "label_set = sorted(\n",
    "    set(label for sentence in train_label_seq for label in sentence),\n",
    "    key=lambda x: (sort_labels_func(x), x),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e934f16-4dd6-4b0f-aa41-7ee2293a5d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-AGE',\n",
       " 'B-AWARD',\n",
       " 'B-CITY',\n",
       " 'B-COUNTRY',\n",
       " 'B-CRIME',\n",
       " 'B-DATE',\n",
       " 'B-DISEASE',\n",
       " 'B-DISTRICT',\n",
       " 'B-EVENT',\n",
       " 'B-FACILITY',\n",
       " 'B-FAMILY',\n",
       " 'B-IDEOLOGY',\n",
       " 'B-LANGUAGE',\n",
       " 'B-LAW',\n",
       " 'B-LOCATION',\n",
       " 'B-MONEY',\n",
       " 'B-NATIONALITY',\n",
       " 'B-NUMBER',\n",
       " 'B-ORDINAL',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-PENALTY',\n",
       " 'B-PERCENT',\n",
       " 'B-PERSON',\n",
       " 'B-PRODUCT',\n",
       " 'B-PROFESSION',\n",
       " 'B-RELIGION',\n",
       " 'B-STATE_OR_PROVINCE',\n",
       " 'B-TIME',\n",
       " 'B-WORK_OF_ART',\n",
       " 'I-AGE',\n",
       " 'I-AWARD',\n",
       " 'I-CITY',\n",
       " 'I-COUNTRY',\n",
       " 'I-CRIME',\n",
       " 'I-DATE',\n",
       " 'I-DISEASE',\n",
       " 'I-DISTRICT',\n",
       " 'I-EVENT',\n",
       " 'I-FACILITY',\n",
       " 'I-FAMILY',\n",
       " 'I-IDEOLOGY',\n",
       " 'I-LANGUAGE',\n",
       " 'I-LAW',\n",
       " 'I-LOCATION',\n",
       " 'I-MONEY',\n",
       " 'I-NATIONALITY',\n",
       " 'I-NUMBER',\n",
       " 'I-ORDINAL',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-PENALTY',\n",
       " 'I-PERCENT',\n",
       " 'I-PERSON',\n",
       " 'I-PRODUCT',\n",
       " 'I-PROFESSION',\n",
       " 'I-RELIGION',\n",
       " 'I-STATE_OR_PROVINCE',\n",
       " 'I-TIME',\n",
       " 'I-WORK_OF_ART']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e23f3f1-b1f3-4df0-89d4-6d84baf1ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label2idx(label_set: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from labels to indices.\n",
    "    \"\"\"\n",
    "\n",
    "    label2idx: Dict[str, int] = {}\n",
    "    for idx, label in enumerate(label_set):\n",
    "        label2idx[label] = idx\n",
    "\n",
    "    return label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37baf550-46a0-4f89-86e3-a2190a044c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = get_label2idx(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24ef7a32-d8ad-4daa-94ff-9284b06c0b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\t0\n",
      "<UNK>\t1\n",
      "Египет\t2\n",
      "ского\t3\n",
      "студента\t4\n",
      "могут\t5\n",
      "выслать\t6\n",
      "из\t7\n",
      "страны\t8\n",
      "за\t9\n"
     ]
    }
   ],
   "source": [
    "for token, idx in list(token2idx.items())[:10]:\n",
    "    print(f\"{token}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97066bb-af38-4d83-a1c1-6879329b078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t0\n",
      "B-AGE\t1\n",
      "B-AWARD\t2\n",
      "B-CITY\t3\n",
      "B-COUNTRY\t4\n",
      "B-CRIME\t5\n",
      "B-DATE\t6\n",
      "B-DISEASE\t7\n",
      "B-DISTRICT\t8\n",
      "B-EVENT\t9\n"
     ]
    }
   ],
   "source": [
    "for label, idx in list(label2idx.items())[:10]:\n",
    "    print(f\"{label}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6711bb5-0abc-4a23-afed-f1fa5ceb92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс датасета NERDataset\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: List[List[str]],\n",
    "        label_seq: List[List[str]],\n",
    "        token2idx: Dict[str, int],\n",
    "        label2idx: Dict[str, int],\n",
    "    ):\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
    "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        first_item = torch.LongTensor(self.token_seq[idx])\n",
    "        second_item = torch.LongTensor(self.label_seq[idx])\n",
    "        return first_item, second_item\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_tokens(\n",
    "        tokens: List[str],\n",
    "        token2idx: Dict[str, int],\n",
    "        unk: str = \"<UNK>\",\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "        \"\"\"\n",
    "        idxs = []\n",
    "        for token in tokens:\n",
    "            key = unk\n",
    "            if token in token2idx:\n",
    "                key = token\n",
    "            idx = token2idx[key]\n",
    "            idxs.append(idx)\n",
    "        return idxs\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        idxs = []\n",
    "        for label in labels:\n",
    "            idx = label2idx[label]\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3686866-a582-4e32-9c66-813d308eb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a198c71-d572-4f6d-9d91-60e48c6e4764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  12,\n",
       "           2,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  17,  20,  25,\n",
       "          17,  26,  27,  28,  29,  17,  30,  31,  32,  20,  33,  34,  35,  17,\n",
       "          36,  37,  38,  39,  11,  40,  41,  20,  42,  43,  44,  45,  46,  20,\n",
       "          47,  48,  49,  17,  50,  51,  52,  20,  53,  54,  55,  20,  56,  57,\n",
       "          58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  17,  70,\n",
       "          71,  72,  16,   7,  73,  74,  17,  75,  76,  33,  77,  78,  20,  79,\n",
       "          71,  80,  17,  81,  82,  83,  84,  85,  86,  87,  88,  20,  89,  17,\n",
       "          90,  91,  92,  93,  94,  95,  96,   9,  97,  61,  45,  98,  17,  90,\n",
       "          99,  37, 100, 101,   9, 102,  62, 103,  33, 104, 105, 106,  67,  68,\n",
       "         107,  20, 108, 109, 110, 111, 112, 113, 114,  17,  36,  84, 115,  43,\n",
       "         116, 117,  62, 118, 119, 120, 121, 122,  67,  68, 107,  17, 123, 124,\n",
       "          17,  90,  40, 125, 126, 127, 128, 129, 130,  62, 131,  82,  83,  20,\n",
       "         132, 117,  67,  68,  69, 133, 134, 135,  17,  40, 136, 137, 138, 139,\n",
       "          23, 140,  62, 141, 142,  17,  90, 143,  20, 144, 145,  17, 146, 147,\n",
       "         148,  17, 101, 149, 127,  37,  17, 150,  40, 151, 152, 153,  37, 154,\n",
       "         155,  62, 156,  22,  23, 157, 119, 120, 121,  17,  90, 158, 159, 160,\n",
       "         105,  84, 161,  67,  68, 107, 162, 163,  40, 164,  62, 165, 166, 167,\n",
       "          17,  90,  91,  20, 168, 169, 170,  67,  68, 107,  17, 171,  37, 172,\n",
       "         173, 174,  57, 105,  62,  67,  68,  69, 167,  17,  90, 133, 175, 135,\n",
       "          17,  81,  82,  83, 176, 177, 178, 179, 180, 181, 182,  20, 183, 184,\n",
       "          62,  67,  68,  69, 167,  17,  90,  31, 185, 186, 187,  10, 188,  61,\n",
       "          17, 150, 127, 189, 129, 190, 191, 192, 193,  62]),\n",
       " tensor([17, 46, 25,  0, 21, 50, 50,  0,  0,  0, 23, 52, 23, 52, 17, 46, 25,  0,\n",
       "          0,  0,  0,  4,  0,  0,  0,  0,  6, 35, 35, 35, 35, 35, 35,  0,  0,  0,\n",
       "          0,  0, 20, 49, 49,  0,  0,  0,  0,  0,  0,  0, 21,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25, 54, 54,  0, 20, 49,\n",
       "         23, 52,  0, 23, 52, 52, 52, 52, 52, 52,  0,  1, 30, 30, 25,  0,  3, 32,\n",
       "          0,  0,  0, 20, 49, 49,  0,  3, 32, 32,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0, 24,  0,  0,  0,  0,  0, 21, 50, 50,  0,  0, 23,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23, 52, 52,  0, 10, 39,\n",
       "         39, 39, 39, 39, 39,  0,  0,  0,  0,  6, 35, 35,  0,  0,  0,  0,  0, 25,\n",
       "         23, 52, 52,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  6, 35, 35, 23, 52, 52,  0,  0,  0,  0,  0,  0,  0, 25, 54, 54, 54,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0, 23, 52, 52,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0, 23, 52, 52,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0, 23, 52, 52,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0, 26,  0,  4, 33,  0, 23, 52, 52,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8daf929e-b6fc-4319-81d5-dcddb753c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_padding_value: int,\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[torch.LongTensor, torch.LongTensor]],\n",
    "    ) -> Dict[str, torch.LongTensor]:\n",
    "\n",
    "        tokens, labels = zip(*batch)\n",
    "        tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True, padding_value=self.token_padding_value).long()\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.label_padding_value).long()\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(list(map(partial(torch.ones_like, dtype=torch.bool), tokens)), batch_first=True, padding_value=False).long()\n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01213752-0902-493a-b144-8b9c02cde45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = NERCollator(\n",
    "    token_padding_value=tokenizer.pad_token_id,\n",
    "    label_padding_value=-100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c48a4-3f90-4c08-831f-8d6a9339ee76",
   "metadata": {},
   "source": [
    "## Обучаемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "764b597f-0dc5-46fc-a684-13c523e3172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31cc6a45-b472-43ff-a7bf-5644b061cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "label_list = []\n",
    "for label in label_set:\n",
    "    if label != 'O':\n",
    "        label_list.append(label2idx[label])\n",
    "\n",
    "def compute_metrics(\n",
    "    evaluation_results: EvalPrediction,\n",
    "    category_id_mapping: Dict[int, str],\n",
    "    no_entity_category_id: int,\n",
    "    short_output = False,\n",
    "    BIO_NOTATION = False\n",
    "    ) -> Dict[str, float]:\n",
    "\n",
    "    predictions = np.argmax(evaluation_results.predictions, axis=-1)\n",
    "    padding_mask = label_mask = (evaluation_results.label_ids != -100)\n",
    "\n",
    "    #label_mask = np.triu(padding_mask)\n",
    "    label_ids = evaluation_results.label_ids[label_mask]\n",
    "    predictions = predictions[label_mask]\n",
    "\n",
    "    unique_label_ids = set(np.unique(label_ids[label_ids != no_entity_category_id]))\n",
    "\n",
    "    labels = sorted(category_id_mapping.keys())\n",
    "    f1_category_scores = f1_score(label_ids, predictions, average=None, labels=labels, zero_division=0)\n",
    "    recall_category_scores = recall_score(label_ids, predictions, average=None, labels=label_list, zero_division=0)\n",
    "    precision_category_scores = precision_score(label_ids, predictions, average=None, labels=label_list, zero_division=0)\n",
    "\n",
    "    results: Dict[str, float] = {}\n",
    "    sum_f1 = 0\n",
    "    sum_recall = 0\n",
    "    sum_precision = 0\n",
    "    for category_id, (f1, recall, precision) in enumerate(zip(f1_category_scores, recall_category_scores, precision_category_scores)):\n",
    "        if category_id == no_entity_category_id:\n",
    "            logger.info(f'O: {f1}, {recall}, {precision}')\n",
    "            continue\n",
    "\n",
    "        if category_id not in unique_label_ids:\n",
    "            logger.info(f'Skipping {category_id_mapping[category_id]}: {f1}, {recall}, {precision}')\n",
    "\n",
    "        category = category_id_mapping[category_id]\n",
    "\n",
    "        sum_f1 += f1\n",
    "        sum_recall += recall\n",
    "        sum_precision += precision\n",
    "\n",
    "    num_categories = len(category_id_mapping) - 1\n",
    "\n",
    "    results['F1_macro'] = sum_f1 / num_categories\n",
    "    results['Recall_macro'] = sum_recall / num_categories\n",
    "    results['Precision_macro'] = sum_precision / num_categories\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47ca3580-37f7-41cb-92b5-475be5bde988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9127312c-0790-4c13-9003-9ce744e72854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/xellar/Projects/NERtask/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 746\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18650\n",
      "  Number of trainable parameters = 29114579\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18650' max='18650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18650/18650 03:45, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.444000</td>\n",
       "      <td>1.282482</td>\n",
       "      <td>0.185399</td>\n",
       "      <td>0.169085</td>\n",
       "      <td>0.315646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.755700</td>\n",
       "      <td>1.144688</td>\n",
       "      <td>0.324784</td>\n",
       "      <td>0.292870</td>\n",
       "      <td>0.415076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>1.216442</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.366108</td>\n",
       "      <td>0.528281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>1.317605</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>0.394883</td>\n",
       "      <td>0.513976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>1.412639</td>\n",
       "      <td>0.463209</td>\n",
       "      <td>0.418210</td>\n",
       "      <td>0.536814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>1.461740</td>\n",
       "      <td>0.491598</td>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.612122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>1.545399</td>\n",
       "      <td>0.496341</td>\n",
       "      <td>0.439796</td>\n",
       "      <td>0.606107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>1.584387</td>\n",
       "      <td>0.498134</td>\n",
       "      <td>0.447723</td>\n",
       "      <td>0.585910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>1.615715</td>\n",
       "      <td>0.498469</td>\n",
       "      <td>0.455271</td>\n",
       "      <td>0.578472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>1.692062</td>\n",
       "      <td>0.497571</td>\n",
       "      <td>0.452340</td>\n",
       "      <td>0.567101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>1.696079</td>\n",
       "      <td>0.488642</td>\n",
       "      <td>0.440407</td>\n",
       "      <td>0.573107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>1.732904</td>\n",
       "      <td>0.489134</td>\n",
       "      <td>0.450403</td>\n",
       "      <td>0.553261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.777433</td>\n",
       "      <td>0.512599</td>\n",
       "      <td>0.476568</td>\n",
       "      <td>0.584212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>1.863016</td>\n",
       "      <td>0.500449</td>\n",
       "      <td>0.459046</td>\n",
       "      <td>0.573178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>1.846565</td>\n",
       "      <td>0.503429</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.592624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>1.939926</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.477098</td>\n",
       "      <td>0.571911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.959970</td>\n",
       "      <td>0.506727</td>\n",
       "      <td>0.467481</td>\n",
       "      <td>0.586554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>1.981852</td>\n",
       "      <td>0.510651</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.592988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.971276</td>\n",
       "      <td>0.512872</td>\n",
       "      <td>0.470071</td>\n",
       "      <td>0.587555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>2.034355</td>\n",
       "      <td>0.524603</td>\n",
       "      <td>0.482572</td>\n",
       "      <td>0.599562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>2.003759</td>\n",
       "      <td>0.500435</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>2.075065</td>\n",
       "      <td>0.512723</td>\n",
       "      <td>0.471964</td>\n",
       "      <td>0.596782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>2.116911</td>\n",
       "      <td>0.519421</td>\n",
       "      <td>0.472435</td>\n",
       "      <td>0.614298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>2.077043</td>\n",
       "      <td>0.516172</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>0.607383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>2.071922</td>\n",
       "      <td>0.521165</td>\n",
       "      <td>0.482811</td>\n",
       "      <td>0.601985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>2.124543</td>\n",
       "      <td>0.535232</td>\n",
       "      <td>0.480318</td>\n",
       "      <td>0.642928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>2.138894</td>\n",
       "      <td>0.524403</td>\n",
       "      <td>0.487190</td>\n",
       "      <td>0.597609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.127044</td>\n",
       "      <td>0.528415</td>\n",
       "      <td>0.479964</td>\n",
       "      <td>0.621009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.167725</td>\n",
       "      <td>0.528807</td>\n",
       "      <td>0.492132</td>\n",
       "      <td>0.604248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>2.145788</td>\n",
       "      <td>0.530919</td>\n",
       "      <td>0.500695</td>\n",
       "      <td>0.598790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>2.165046</td>\n",
       "      <td>0.528086</td>\n",
       "      <td>0.493707</td>\n",
       "      <td>0.603689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.192436</td>\n",
       "      <td>0.524224</td>\n",
       "      <td>0.488607</td>\n",
       "      <td>0.605516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.208922</td>\n",
       "      <td>0.525942</td>\n",
       "      <td>0.491310</td>\n",
       "      <td>0.608150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.193007</td>\n",
       "      <td>0.528577</td>\n",
       "      <td>0.489243</td>\n",
       "      <td>0.614760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.195385</td>\n",
       "      <td>0.531640</td>\n",
       "      <td>0.488029</td>\n",
       "      <td>0.621481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.196126</td>\n",
       "      <td>0.531534</td>\n",
       "      <td>0.492674</td>\n",
       "      <td>0.617271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.197723</td>\n",
       "      <td>0.531550</td>\n",
       "      <td>0.492643</td>\n",
       "      <td>0.617286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-500\n",
      "Configuration saved in model_out/checkpoint-500/config.json\n",
      "Model weights saved in model_out/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-1000\n",
      "Configuration saved in model_out/checkpoint-1000/config.json\n",
      "Model weights saved in model_out/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-1500\n",
      "Configuration saved in model_out/checkpoint-1500/config.json\n",
      "Model weights saved in model_out/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-2000\n",
      "Configuration saved in model_out/checkpoint-2000/config.json\n",
      "Model weights saved in model_out/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-2500\n",
      "Configuration saved in model_out/checkpoint-2500/config.json\n",
      "Model weights saved in model_out/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-3000\n",
      "Configuration saved in model_out/checkpoint-3000/config.json\n",
      "Model weights saved in model_out/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-3500\n",
      "Configuration saved in model_out/checkpoint-3500/config.json\n",
      "Model weights saved in model_out/checkpoint-3500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-4000\n",
      "Configuration saved in model_out/checkpoint-4000/config.json\n",
      "Model weights saved in model_out/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-4500\n",
      "Configuration saved in model_out/checkpoint-4500/config.json\n",
      "Model weights saved in model_out/checkpoint-4500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-5000\n",
      "Configuration saved in model_out/checkpoint-5000/config.json\n",
      "Model weights saved in model_out/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-5500\n",
      "Configuration saved in model_out/checkpoint-5500/config.json\n",
      "Model weights saved in model_out/checkpoint-5500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-6000\n",
      "Configuration saved in model_out/checkpoint-6000/config.json\n",
      "Model weights saved in model_out/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-6500\n",
      "Configuration saved in model_out/checkpoint-6500/config.json\n",
      "Model weights saved in model_out/checkpoint-6500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-7000\n",
      "Configuration saved in model_out/checkpoint-7000/config.json\n",
      "Model weights saved in model_out/checkpoint-7000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-7500\n",
      "Configuration saved in model_out/checkpoint-7500/config.json\n",
      "Model weights saved in model_out/checkpoint-7500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-8000\n",
      "Configuration saved in model_out/checkpoint-8000/config.json\n",
      "Model weights saved in model_out/checkpoint-8000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-8500\n",
      "Configuration saved in model_out/checkpoint-8500/config.json\n",
      "Model weights saved in model_out/checkpoint-8500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-9000\n",
      "Configuration saved in model_out/checkpoint-9000/config.json\n",
      "Model weights saved in model_out/checkpoint-9000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-9500\n",
      "Configuration saved in model_out/checkpoint-9500/config.json\n",
      "Model weights saved in model_out/checkpoint-9500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-10000\n",
      "Configuration saved in model_out/checkpoint-10000/config.json\n",
      "Model weights saved in model_out/checkpoint-10000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-10500\n",
      "Configuration saved in model_out/checkpoint-10500/config.json\n",
      "Model weights saved in model_out/checkpoint-10500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-11000\n",
      "Configuration saved in model_out/checkpoint-11000/config.json\n",
      "Model weights saved in model_out/checkpoint-11000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-11500\n",
      "Configuration saved in model_out/checkpoint-11500/config.json\n",
      "Model weights saved in model_out/checkpoint-11500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-12000\n",
      "Configuration saved in model_out/checkpoint-12000/config.json\n",
      "Model weights saved in model_out/checkpoint-12000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-12500\n",
      "Configuration saved in model_out/checkpoint-12500/config.json\n",
      "Model weights saved in model_out/checkpoint-12500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-13000\n",
      "Configuration saved in model_out/checkpoint-13000/config.json\n",
      "Model weights saved in model_out/checkpoint-13000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-13500\n",
      "Configuration saved in model_out/checkpoint-13500/config.json\n",
      "Model weights saved in model_out/checkpoint-13500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-14000\n",
      "Configuration saved in model_out/checkpoint-14000/config.json\n",
      "Model weights saved in model_out/checkpoint-14000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-14500\n",
      "Configuration saved in model_out/checkpoint-14500/config.json\n",
      "Model weights saved in model_out/checkpoint-14500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-15000\n",
      "Configuration saved in model_out/checkpoint-15000/config.json\n",
      "Model weights saved in model_out/checkpoint-15000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-15500\n",
      "Configuration saved in model_out/checkpoint-15500/config.json\n",
      "Model weights saved in model_out/checkpoint-15500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-16000\n",
      "Configuration saved in model_out/checkpoint-16000/config.json\n",
      "Model weights saved in model_out/checkpoint-16000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-16500\n",
      "Configuration saved in model_out/checkpoint-16500/config.json\n",
      "Model weights saved in model_out/checkpoint-16500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-17000\n",
      "Configuration saved in model_out/checkpoint-17000/config.json\n",
      "Model weights saved in model_out/checkpoint-17000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-17500\n",
      "Configuration saved in model_out/checkpoint-17500/config.json\n",
      "Model weights saved in model_out/checkpoint-17500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-18000\n",
      "Configuration saved in model_out/checkpoint-18000/config.json\n",
      "Model weights saved in model_out/checkpoint-18000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 94\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model_out/checkpoint-18500\n",
      "Configuration saved in model_out/checkpoint-18500/config.json\n",
      "Model weights saved in model_out/checkpoint-18500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18650, training_loss=0.0953935909834528, metrics={'train_runtime': 226.2191, 'train_samples_per_second': 164.884, 'train_steps_per_second': 82.442, 'total_flos': 200561311362984.0, 'train_loss': 0.0953935909834528, 'epoch': 50.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from functools import partial\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=len(label_set)).to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"model_out\",\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    full_determinism=False,\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    ")\n",
    "\n",
    "label_from_idx: Dict[int, str] = {}\n",
    "\n",
    "for idx, label in enumerate(label_set):\n",
    "    label_from_idx[idx] = label\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=partial(\n",
    "        compute_metrics,\n",
    "        category_id_mapping=label_from_idx,\n",
    "        no_entity_category_id=-1,\n",
    "        short_output=True,\n",
    "        BIO_NOTATION=True\n",
    "    )\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "310f2b47-7c76-4aa6-b03a-672adf593cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "\n",
    "trained_model = unwrap_model(trainer.model_wrapped)\n",
    "trained_model = trained_model.to(\"cpu\")\n",
    "\n",
    "with open('rubert_tiny.pkl', 'wb') as handle:\n",
    "    pickle.dump(trained_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc58742a-4be5-4f7b-8d95-a080ae0aaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label2idx.pkl', 'wb') as handle:\n",
    "    pickle.dump(label2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('token2idx.pkl', 'wb') as handle:\n",
    "    pickle.dump(token2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc488060-9662-4522-88be-7283c64fc7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import pickle\n",
    " \n",
    "def compress_data(data):\n",
    "    return lzma.compress(pickle.dumps(data), format=lzma.FORMAT_RAW, filters=[{\"id\":lzma.FILTER_LZMA2,\"dict_size\":268435456, \"preset\":9, \"mf\":lzma.MF_HC3, \"depth\":0, \"lc\":3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ee37c4-7ff4-4ee1-a33f-516f85beffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_state = compress_data(trained_model.state_dict())\n",
    "compressed_config = compress_data(trained_model.config)\n",
    "f = open(\"state.xz\", \"wb\")\n",
    "f.write(compressed_state)\n",
    "f.close()\n",
    "f = open(\"config.xz\", \"wb\")\n",
    "f.write(compressed_config)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d85f94dc-1a3f-459e-a2ed-1aa2ab38e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 211M\n",
      "drwxrwxr-x 11 xellar xellar 4.0K Nov 28 10:29 .\n",
      "drwxrwxr-x  4 xellar xellar 4.0K Nov 26 16:29 ..\n",
      "-rw-rw-r--  1 xellar xellar 1.4K Nov 28 10:30 config.xz\n",
      "drwxrwxr-x  5 xellar xellar 4.0K Nov 26 17:48 data\n",
      "-rw-rw-r--  1 xellar xellar 1.9K Nov 27 19:25 dataset.py\n",
      "drwxrwxr-x  2 xellar xellar 4.0K Nov 27 18:44 .ipynb_checkpoints\n",
      "-rw-rw-r--  1 xellar xellar  842 Nov 28 10:27 label2idx.pkl\n",
      "drwxrwxr-x 39 xellar xellar 4.0K Nov 28 10:27 model_out\n",
      "-rw-rw-r--  1 xellar xellar 143K Nov 28 10:29 NER_notebook.ipynb\n",
      "drwxrwxr-x  2 xellar xellar 4.0K Nov 27 10:43 out\n",
      "drwxrwxr-x  2 xellar xellar 4.0K Nov 27 19:33 __pycache__\n",
      "drwxrwxr-x  3 xellar xellar 4.0K Nov 27 12:20 res\n",
      "-rw-rw-r--  1 xellar xellar 8.8K Nov 27 18:54 Rubert.ipynb\n",
      "-rw-rw-r--  1 xellar xellar 112M Nov 28 10:27 rubert_tiny.pkl\n",
      "-rw-rw-r--  1 xellar xellar 4.4K Nov 28 09:58 solution.py\n",
      "-rw-rw-r--  1 xellar xellar  99M Nov 28 10:30 state.xz\n",
      "drwxrwxr-x  2 xellar xellar 4.0K Nov 27 09:12 tmp_trainer\n",
      "-rw-rw-r--  1 xellar xellar 581K Nov 28 10:27 token2idx.pkl\n",
      "drwxrwxr-x  2 xellar xellar 4.0K Nov 27 20:43 tokenizer\n",
      "-rw-rw-r--  1 xellar xellar  123 Nov 27 19:26 utils.py\n",
      "drwxrwxr-x  7 xellar xellar 4.0K Nov 26 17:52 venv\n"
     ]
    }
   ],
   "source": [
    "! ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5142f-b020-4bf2-ad4c-ead6dfe61a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
